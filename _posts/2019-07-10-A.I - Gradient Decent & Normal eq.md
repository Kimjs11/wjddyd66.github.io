---
layout: post
title:  "A.I-Linear Regression"
date:   2019-07-10 10:00:00 +0700
categories: [A.I]
---

### Gradient Decent & Normal eq
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

<span style ="color: red">정규방정식(Normal equation 혹은 Ordinary least squares 혹은 linear least squares)은 통계학에서 선형 회귀상에서 알지 못하는 값(parameter)를 예측하기 위한 방법론이다. </span>  
<span style ="color: red">**경사 하강법이 수학적 최적화 알고리즘으로서 적절한 학습비율(learning rate)를 설정해야하고 많은 연산량이 필요**</span>하지만 정규방정식에는 그와 같은 단점이 없다는 장점이 있다. 하지만 정규방정식은 <span style ="color: red">**행렬 연산에 기반하기 때문에 피쳐의 개수가 엄청나게 많을 경우 연산이 느려지는 것**</span>을 피할 수 없다. 하지만 <span style ="color: red">**경사 하강법은 아무리 많은 피쳐가 존재하더라도 일정한 시간 내에 해법을 찾는 것이 가능**</span>하다. 그러므로 예측 알고리즘을 선택할 때 있어 <span style ="color: red">**피쳐의 개수**</span>에 따라 알맞은 것을 선택하여야 한다.  
### Normal eq
$$y= a X + b$$  
라는 식이 있을경우 이것을 행렬로서 표현할 수 있다.  
$$\begin{bmatrix} [y] end{bmatrix} = \begin{bmatrix} [x] & 1 \begin{bmatrix} a \\ b \end{bmatrix}$$  

위의 식을 풀어서 쓰게 되면 아래와 같은 식이 된다.  
$$\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ ... \\ y_n \\end{bmatrix} = \begin{bmatrix} x_1 & 1 \\ x_2 & 1 \\ x_3 & 1 \\ x_n & 1 \\ \begin{bmatrix} a \\ b \end{bmatrix}$$  

위의 식은 아래와 같은 그래프로서 표현할 수 있다.  

<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/1.png" height="200" width="600" /></div>
실제값과 예측값의 차이는 파란색선의 길이의 합이다.  
실제값과 예측값의 차이는 Cost라고 불리게 되고 이러한 Cost에 대한 식은 아래 식으로 나타낼 수 있다.  

$$f_c(x)=\sum_{i=0}^n  (y_i-\hat{y_i})^2$$  
Cost Function은 아래와 같은 그림으로 나타낼 수 있다.  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/2.PNG" height="250" width="600" /></div>
Cost가 0이 될 확률을 매우 낮지만 <span style ="color: red">**0에 가까울 때(미분했을때의 기울기가 0인 점)**</span>의 값을 구하는 것이 Cost를 최소로 할 수 있다.  
간단하게 구할 수 있다고 생각하지만 Weight가 많아지게 되면 수식이 복잡하게 되므로 Gradient Decent를 사용하게 된다.  

### Gradient Decent
Gradient Decent는 Cost Function을 W에 애해 편미분하면 현재 W위치에서의 접선의 기울기와 같다.  
이러한 W값에서 어떤 음수만큼 빼주게 되어 더하게 된다.  
$$W(update)=w-a\frac{\partial f_c(x)}{\partial W}$$  
즉 W값이 점점 커지면서 새롭게 갱신된 W에 대해서 위와같은 공식을 반복적으로 적용한다.  
<span style ="color: red">**주의해야 할 점은 a(학습률 파라미터 = Learning Rate)를 적절한 값으로 설정해줘야 한다는 것이다.**</span><br>
학습률 파라미터가 너무 작은 값이면 최적의 w를 찾아가는데 너무 오래 걸릴 가능성이 크고, 너무 크면 최적의 지점을 건너뛰어 버리고 발산해 버릴 수 있다.  

<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/3.PNG" height="250" width="600" /></div>
계속하여 W를 갱신하여 Cost값이 최소가 되는(미분값이 0 인) 곳을 찾는다.  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/4.PNG" height="250" width="600" /></div>
<hr>
참조: <a href="https://www.youtube.com/watch?v=M9Gsi3VBTYM&list=PL1H8jIvbSo1q6PIzsWQeCLinUj_oPkLjc&index=22">Chanwoo Timothy Lee Youtube</a> <br>
참조: <a href="https://ko.wikipedia.org/wiki/%EC%A0%95%EA%B7%9C%EB%B0%A9%EC%A0%95%EC%8B%9D">나무위키</a> <br>
문제가 있거나 궁금한 점이 있으면 wjddyd66@naver.com으로  Mail을 남겨주세요.