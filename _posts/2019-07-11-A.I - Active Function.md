---
layout: post
title:  "A.I-Active Function"
date:   2019-07-11 09:00:00 +0700
categories: [AI]
---

### Active Function
<span style ="color: red">**1. 시그모이드**</span><br>
식: <span> $$\sigma(x) = {1 \over e^{-x}}$$ </span><br>
미분식: <span> $$\prime{\sigma(x)} = \sigma(x)(1-\sigma(x))$$ </span>
<br>
범위:[0,1]  
시그모이드 함수 그래프  
<div><img src="http://i.imgur.com/HpSpWal.png" height="100%" width="100%" /></div>
시그모이드 미분 그래프  
<div><img src="http://i.imgur.com/WpKD6kW.png" height="100%" width="100%" /></div>
<span style ="color: red">**-5 보다 작거나 5 보다 클 경우**</span>Gradient값이 지나치게 작아지고 exp연산때문에 느려지는 단점이 생기게 된다.<br>
<span style ="color: red">**항상 0이상의 값**</span>을 가지기 때문에 Gradient Decent로 w를 학습시 허용되는 방향에 제약이 가해져 학습속도가 늦거나 수렴이 어렵게 된다.
<span style ="color: red">**2. 하이퍼 볼릭 탄젠트**</span><br>
식: <span> $$\tanh(x) = {e^{x} - e^{-x} \overe^{x} + e^{-x}}$$ </span><br>
미분식: <span> $$\prime{\tanh(x)} = 1-\tanh^2(x)$$ </span>
<br>
범위:[-1,1]  
하이퍼 볼릭 탄젠트 함수 그래프  
<div><img src="http://i.imgur.com/xaQpDt4.png" height="100%" width="100%" /></div>
하이퍼 볼릭 탄젠트 미분 그래프  
<div><img src="http://i.imgur.com/0mVuW9h.png" height="100%" width="100%" /></div>
<span style ="color: red">**-5 보다 작거나 5 보다 클 경우**</span>Gradient값이 0으로 가까워 진다는 단점이 존재하게 된다.<br>
<span style ="color: red">**0을 기준으로 대칭되는 값**</span>을 가지기 때문에 Gradient Decent로 w를 학습시 허용되는 방향에 제약이없어져 시그모이드보다 학습속도가 빠르고 수렴이 쉽게 된다.  
<span style ="color: red">**3. ReLU**</span><br>
식: <span> $$f(x) = max(0,x)$$ </span><br>
미분식: <span> $$\prime{f(x)} = \begin{cases}
1, & \mbox{if }n\mbox{ > 0} \\
0, & \mbox{if }n\mbox{ < 0}
\end{cases}$$ </span>
<br>
범위:0이상의 양수  
ReLU 함수 그래프  
<div><img src="http://i.imgur.com/SAxRPcy.png" height="100%" width="100%" /></div>
<span style ="color: red">**5 보다 클 경우**</span>Gradient값이 0으로 가까워 진다는 단점을 극복하였으나 <span style ="color: red">**0 보다 작을 경우**</span>모든 값을 0이 된다는 단점이 존재하게 된다.<br>
<hr>
참조: <a href="https://ratsgo.github.io/deep%20learning/2017/04/22/NNtricks/">ratsgo 블로그</a> <br>
문제가 있거나 궁금한 점이 있으면 wjddyd66@naver.com으로  Mail을 남겨주세요.