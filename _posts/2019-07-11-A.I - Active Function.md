---
layout: post
title:  "A.I-Active Function"
date:   2019-07-11 09:00:00 +0700
categories: [AI]
---

### Active Function
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
<span style ="color: red">**1. 시그모이드**</span><br>
식: <span> $$\sigma(x) = {1 \over e^{-x}}$$ </span><br>
미분식: <span> $$\sigma\prime(x) = \sigma(x)(1-\sigma(x))$$ </span>
<br>
범위:[0,1]  
시그모이드 함수 그래프  
<div><img src="http://i.imgur.com/HpSpWal.png" height="100%" width="100%" /></div>
시그모이드 미분 그래프  
<div><img src="http://i.imgur.com/WpKD6kW.png" height="100%" width="100%" /></div>
<span style ="color: red">**-5 보다 작거나 5 보다 클 경우**</span>Gradient값이 지나치게 작아지고 exp연산때문에 느려지는 단점이 생기게 된다.<br>
<span style ="color: red">**항상 0이상의 값**</span>을 가지기 때문에 Gradient Decent로 w를 학습시 허용되는 방향에 제약이 가해져 학습속도가 늦거나 수렴이 어렵게 된다.  
<br>
<span style ="color: red">**2. 하이퍼 볼릭 탄젠트**</span><br>
식: <span> $$\tanh(x) = {e^{x} - e^{-x} \over e^{x} + e^{-x} }$$ </span><br>
미분식: <span> $$\tanh\prime(x) = 1-\tanh^2(x)$$ </span>
<br>
범위:[-1,1]  
하이퍼 볼릭 탄젠트 함수 그래프  
<div><img src="http://i.imgur.com/xaQpDt4.png" height="100%" width="100%" /></div>
하이퍼 볼릭 탄젠트 미분 그래프  
<div><img src="http://i.imgur.com/0mVuW9h.png" height="100%" width="100%" /></div>
<span style ="color: red">**-5 보다 작거나 5 보다 클 경우**</span>Gradient값이 0으로 가까워 진다는 단점이 존재하게 된다.<br>
<span style ="color: red">**0을 기준으로 대칭되는 값**</span>을 가지기 때문에 Gradient Decent로 w를 학습시 허용되는 방향에 제약이없어져 시그모이드보다 학습속도가 빠르고 수렴이 쉽게 된다.  
<br>
<span style ="color: red">**3. ReLU**</span><br>
식: <span> $$f(x) = max(0,x)$$ </span><br>
미분식:
 - x > 0 : 1
 - x < 0 : 0


범위:0이상의 양수  
ReLU 함수 그래프  
<div><img src="http://i.imgur.com/SAxRPcy.png" height="100%" width="100%" /></div>
<span style ="color: red">**5 보다 클 경우**</span>Gradient값이 0으로 가까워 진다는 단점을 극복하였으나 <span style ="color: red">**0 보다 작을 경우**</span>모든 값을 0이 된다는 단점이 존재하게 된다.<br>

<br>
<span style ="color: red">**4. 소프트맥스**</span><br>
식: <span> $$y = \frac{e^{wx_i}}{\sum_{i=0}^n e^{wx_i}}$$ </span><br>
입력받은 값을 출력으로 0~1사이의 값으로 모두 <span style ="color: red">**정규화**</span>하며 <span style ="color: red">**출력 값들의 총합은 항상 1이 되는 특성**</span>을 가진 함수이다.  
분류하고 싶은 클래수의 수 만큼 출력으로 구성한다.  
아래 그림을 보게 되면 Sigmoid와 차이를 알 수 있다.  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/12.PNG" height="250" width="600" /></div>
<br>
**Sigmoid 사용할 경우** Output 1을 위한 Weight를 Update시키게 되면 위의 그림에서 빨간선 3개만 Update의 대상이 된다.  
하지만 **Softmax를 사용하게 되면** Output1 + Output2 + Output3 =1 이 되므로 Output 1을 위한 Weight를 Update 시키게 되면 자동적으로 Output2, Output3까지 모두 영향을 받게 되므로 <span style ="color: red">**학습의 과속화**</span>가 진행되어 많이 사용하게 된다.  
<hr>
참조: <a href="https://ratsgo.github.io/deep%20learning/2017/04/22/NNtricks/">ratsgo 블로그</a> <br>
문제가 있거나 궁금한 점이 있으면 wjddyd66@naver.com으로  Mail을 남겨주세요.