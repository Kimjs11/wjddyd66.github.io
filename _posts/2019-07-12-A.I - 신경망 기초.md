---
layout: post
title:  "A.I-신경망 기초"
date:   2019-07-12 10:00:00 +0700
categories: [AI]
---

### 신경망 기초
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

하나의 신경은 아래 그림과 같이 많이 표현한다.  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/13.PNG" height="250" width="600" /></div>
x1, x2, x2를 Input(입력)이라 한다.  
y는 Output(출력)이라고 한다.  
w1, w2, w2는 Weight(가중치)라고 한다.  
Activate Function은 들어오는 Input과 가중치를 활용하여 Output을 만들어내기 위한 식이다.  
위의 그림에서 Activate Function이 Sigmoid를 사용한다고 하면 아래와 같은 식을 얻어낼 수 있다.  
<p> $$ y(x) = {1 \over 1+e^{-(w_1x_1+w_2x_2+w_3x_3)}}$$ </p><br>
위의 그림과 같은 Node를 여러개를 연결하여 하나의 신경망을 구성하게 된다.  
신경망은 대부분 아래와 같이 표현할 수 있다.  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/14.PNG" height="250" width="600" /></div>
위와같은 그림은 아래와 같이 행렬로써 표현할 수 있다.  

Input Layer => Hidden Layer  
<p>$$\begin{bmatrix} w_00 & w_01 & w_02 \\w_10 & w_11 & w_12 \\w_20 & w_21 & w_22 \end{bmatrix} \begin{bmatrix} x_0 \\ x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} a\\b\\c \end{bmatrix} => sigmoid => \begin{bmatrix} h_0\\h_1\\h_2 \end{bmatrix}$$</p><br>

Hidden Layer => Output Layer
<p>$$\begin{bmatrix} w_00 & w_01 & w_02 \\w_10 & w_11 & w_12 \end{bmatrix} \begin{bmatrix} h_0 \\ h_1 \\ h_2 \end{bmatrix} = \begin{bmatrix} a\\b \end{bmatrix} => sigmoid => \begin{bmatrix} y_0\\y_1 \end{bmatrix}$$</p><br>



<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/5.png" height="250" width="600" /></div>
b값을 변화시켰을때의 시그모이드 그래프  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/6.png" height="250" width="600" /></div>

### Cross Entropy
Logistic Regression또한 Linear Regression과 같이 MSE를 사용하여 구할 수 있으나 Cross Entropy를 사용하여 Model의 성능을 향상시킬 수 있다.  
Cross Entropy를 이해하기 위해서 <span style ="color: red">**정보량, 엔트로피**</span>의 개념을 알고 있어야 한다.  
<br>
<span style ="color: red">**정보량**</span>은 아래와 같은 식으로 표현할 수 있다.  
<p>$$I(x) = log(\frac{1}{p(x)}) $$ </p><br>
<span>$$(1 \over p(x)) $$ </span>은 사건이 발생할 수 있는 확률이다.  
이러한 값에 log를 취함으로 인하여 <span style ="color: red">**필요한 최소한의 자원**</span>을 나타낸다.  
<br>
<span style ="color: red">**Entropy**</span>는 아래와 같은 식으로 표현할 수 있다.  
<p>$$H_p(X)=\sum_{i=0}^n  p(x_i)log(p(x_i)) $$ </p><br>
Entropy는 <span style ="color: red">**정보량에 대한 기댓값이며 동시에 사건을 표현하기 위해 요구되는 평균 자원이라고 할 수 있다.**</span>으로 정의된다.  
예측이 어려울수록 정보의 양은 더 많아지고 엔트로피는 더 커진다.  
<br>
<span style ="color: red">**Cross Entropy**</span>의식은 아래와 같다.  
<p>$$H_{p,q}(x)=\sum_{i=0}^n  p(x_i)log(q(x_i)) $$ </p><br>
Entropy는 <span style ="color: red">**p는 true label에 대한 분포를, q는 현재 예측모델의 추정값에 대한 분포**</span>를 의미하게 된다.  
위의 식은 아래와 같은 식으로 나타낼 수 있다.  
<p>$$f_c(x)=y\prime log(y) - (1-y\prime)log(1-y) $$ </p><br>
위의 식에서 우변의 값을 각각 따로 그래프로 그려보게 되면 아래와 같다.  
<span>$$y\prime log(y)$$ </span>그래프  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/7.png" height="250" width="600" /></div>
<span>$$(1-y\prime)log(1-y)$$ </span>그래프  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/8.png" height="250" width="600" /></div>
<span> $$ y(x) = {1 \over 1+e^{-ax+b}}$$ </span>식에서 a, b값을 조정함에 따라 위의 그래프들의 값이 아래 그림과 같이 변경된다.  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/9.PNG" height="250" width="600" /></div>
서로 값들이 상봔되는 관계를 가지고 있고 두개의 값을 더했을때 가장 작은 값을 찾는것을 목표로 한다.  
<br>
b에 따른 Cost Function의 식은 아래와 같다.  
<p>$$b(update)=b-\alpha\frac{\partial f_c(x)}{\partial b}$$</p><br>
위의 식을 그래프로 표현하면 아래와 같이 된다고 생각할 수 있다.  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/10.PNG" height="250" width="600" /></div>
a에 따른 Cost Function의 식은 아래와 같다.  
<p>$$a(update)=a-\alpha\frac{\partial f_c(x)}{\partial a}$$</p><br>
위의 식을 그래프로 표현하면 아래와 같이 된다고 생각할 수 있다.  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/11.PNG" height="250" width="600" /></div>
<hr>
참조: <a href="https://www.youtube.com/watch?v=kHLqMsN7yao&list=PL1H8jIvbSo1q6PIzsWQeCLinUj_oPkLjc&index=23">Chanwoo Timothy Lee Youtube</a> <br>
참조: <a href="https://curt-park.github.io/2018-09-19/loss-cross-entropy/">curt-park 블로그</a> <br>
문제가 있거나 궁금한 점이 있으면 wjddyd66@naver.com으로  Mail을 남겨주세요.