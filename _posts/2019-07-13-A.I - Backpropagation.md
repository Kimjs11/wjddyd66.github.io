---
layout: post
title:  "Backpropagation"
date:   2019-07-13 10:00:00 +0700
categories: [AI]
---

### Backpropagation
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

Backpropagation을 알기전에 Chain Rule이라는 것을 먼저 알아야 한다.  
Chain Rule은 합성함수의 미분법이다.  
n변수 함수 <span>$$f(x_1,x_2,x_3,...,x_n)$$</span>에 대해  
<span>$$x_k = g_k(t_1,t_2,t_3,...,t_m) (k=1,2,3,...,n)$$</span>이면  
<span>$$\frac{\partial f}{\partial t_i}=\frac{\partial f}{\partial x_1}\frac{\partial x_1}{\partial t_i}+\frac{\partial f}{\partial x_2}\frac{\partial x_2}{\partial t_i}+...+\frac{\partial f}{\partial x_n}\frac{\partial x_n}{\partial t_i} (i=1,2,3,...,m)$$</span>이다.  
Chain Rule에대한 자세한 내용:<a href="http://blog.naver.com/PostView.nhn?blogId=mindo1103&logNo=90103548178">Nenyaffle 블로그</a>  

아래와 그림과 같이 간단한 하나의 신경을 생각해 보자.  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/15.PNG" height="200" width="600" />
</div>

위와 같은 신경에 대한 Cost Function을 MSE를 사용하면 Cost Function은 아래와 같은 식으로 표현할 수 있다.  
<p>$$C=\frac{1}{2}(y-y\prime)^{2}$$</p>

Weight를 Update하기 위하여 Gradient Decent를 사용하게 되면 W2에 대한 식은 아래와 같은 식으로 표현할 수 있다.  
<p>$$W_2(Update)=W_2-\alpha\frac{\partical C}{\partical W_2}$$</p>

예측한 값인 <span>$$y\prime$$ </span>은 결국 0 혹은 1로써 표현되는 상수이므로 아래와 같은 식을 유도할 수 있다.  
<p>$$\frac{\partical C}{\partical W_2} = (y-y\prime) \frac{\partical y}{\partical W_2} = (y-y\prime) \frac{\partical}{\partical W_2}g(w_2h)  (g(x): Sigmoid Function) $$</p>

Sigmoid 함수를 미분하게 되면 <span>$$g(x)\prime = g(x)(1-g(x))$$ </span>이다.  
또한 <span>$$g(x) = y(1-y)$$ </span>이다.  
<br>
위의 식과 Chain Rule을 사용하게 되면 아래와 같은 식을 얻을 수 있다.  
<p>$$\frac{\partical C}{\partical W_2} = (y-y\prime)g(w_2h)(1-g(w_2h)) \frac{\partical w_2h}{\partical W_2} = (y-y\prime)y(1-y) \frac{\partical w_2h}{\partical W_2} = (y-y\prime)y(1-y)h$$</p>

Weight2를 Update하였으므로 W1에 대한 식은 아래와 같은 식으로 표현할 수 있다.  
<p>$$\frac{\partical C}{\partical W_1} = (y-y\prime) \frac{\partical y}{\partical W_1} = (y-y\prime) \frac{\partical}{\partical W_1} = (y-y\prime)g(w_2h)(1-g(w_2h)) \frac{\particalw_2h}{\partical W_1} = (y-y\prime)y(1-y) \frac{\particalw_2h}{\partical W_1} = (y-y\prime)y(1-y)w_2 \frac{\partical h}{\partical W_1}$$</p>
<hr>
참조: <a href="https://www.youtube.com/watch?v=fhrORKjjU7w&list=PL1H8jIvbSo1q6PIzsWQeCLinUj_oPkLjc&index=25">Chanwoo Timothy Lee Youtube</a> <br>
참조: <a href="http://blog.naver.com/PostView.nhn?blogId=mindo1103&logNo=90103548178">Nenyaffle 블로그</a><br>
문제가 있거나 궁금한 점이 있으면 wjddyd66@naver.com으로  Mail을 남겨주세요.