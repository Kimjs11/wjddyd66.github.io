---
layout: post
title:  "Optimization - Local Minima, Plateau, ZigZag"
date:   2019-07-14 10:00:00 +0700
categories: [AI]
---

### Local Minima
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

Local minima 문제는 에러를 최소화시키는 최적의 파라미터를 찾는 문제에 있어서 아래 그림처럼 파라미터 공간에 수많은 지역적인 홀(hole)들이 존재하여 이러한 local minima에 빠질 경우 전역적인 해(global minimum)를 찾기 힘들게 되는 문제를 일컫는다.  

<div><img src="https://t1.daumcdn.net/cfile/tistory/9965444D5B627B4412" height="200" width="600" />
</div>
그림출처:<a href="https://nittaku.tistory.com/271">nittaku 블로그</a><br>
<span style ="color: red">**실제 딥러닝 모델에서는 Weight가 수도없이 많으며, 그 수많은 Weight가 모두 Local minima에 빠져야 Weight Update가 정지되기 때문에**</span> 불가능하다. Local Minima을 해결하기 위하여 Optimization을 할 이유는 없다.  

### Plateau
Gradient Descent를 타고 Global Optima를 향해서 나아가는데, 평지(Plateau)가 생겨 loss가 업데이트 되지 않는 현상이 발생한다. 이러한 것을 Plateau현상 이라고 한다. 또한 Local Minima에 비해 일어날 확률이 매우 높다.  
<div><img src="https://t1.daumcdn.net/cfile/tistory/9933BB4C5B627B4514" height="200" width="600" />
</div>
그림출처:<a href="https://nittaku.tistory.com/271">nittaku 블로그</a><br>

### ZigZag현상
Weight를 Update 시키기 위한 BackPropagation을 Chain Rule에 적용시킨 결론은 아래와 같았다.  
<p>$$\delta(n-1) = \delta ng\prime(x) W$$</p>
<a href="https://wjddyd66.github.io/ai/2019/07/13/A.I-Backpropagation.html">BackPropagation 자세한 내용</a>  
**Active Function을 Sigmoid나 Relu**를 사용하게 되면, <span>$$\delta n$$</span>(output: 0~1) 및 <span>$$g\prime(x)$$</span>(Sigmoid의 편미분)이 모두 양수이므로 Weight업데이트량은 언제나 + or -가 나오며, 업데이트 방향을 잡을 때, 비효율적으로 ZigZag현상이 발생하여, 업데이트 현상이 느려진다.  

우리가 지금까지 사용해온<span style ="color: red">**Gradient Descent 로서는**</span>  
1. Local Minima
2. Plateau
3. ZigZag현상

위의 3개를 해결할 수 없다.  

### Optimazation
**Gradient Descent**  
매 번 모든 데이터들에 대한 전부 살펴보고 기울기를 계산하므로 시간이 매우 많이 소요되는 문제가 있다.  
 - 뭉뚱한 부분에서 느림
 - Local Minima 현상 
 - Plateau 현상
 - ZigZag 현상


<br>
**Momentum**  
Local Minima에 덜 빠지기 위해 Learning Rate에게 일종의 관성이라 할 수 있는 Momentum을 둔다. 직전에 나온 방향성 즉, 직전에 계산된 기울기를 고려하여 새로 계산된 기울기와 일정한 비율로 계산을 하는 것이다. 이렇게 하면 기울기가 갑자기 양수에서 음수로, 음수에서 양수로 바뀌는 경우가 줄어 들게 되고, 완만한 경사를 더 쉽게 타고 넘을 수 있게 된다.  
하지만 **ZigZag 현상**을 완벽히 해결하지는 못한다.  
<div><img src="https://t1.daumcdn.net/cfile/tistory/9929D1405B629B7635" height="200" width="600" />
</div>
그림출처:<a href="https://nittaku.tistory.com/271">nittaku 블로그</a><br>
아래 수식으로서 간단히 표현할 수 있다.  
<p>$$v \leftarrow \alpha v -  \beta \frac{\partial L}{\partial \theta}$$</p>
<p>$$\theta \leftarrow \theta + v$$</p>
새로운 하이퍼 파라미터인 <span>$$\alpha , v$$</span>가 새롭게 추가되 미분값이 계속하여 v에 더해져서 더욱 큰 값을 갖게되어 Plateau나 뭉뚱한 부분에서느림, Local Minima의 3가지를 해결할 수 있다.  

<br>
**AdaGrad**  
Adagrad(Adaptive Gradinet)는 변수들을 update할 때 각각의 변수마다 step size를 다르게 설정해서 이동하는 방식이다.  
'지금까지 많이 변화하지 않은 변수들은' step size를 크게 하고,  
'지금까지 많이 변화한 변수들은' step size를 작게 하자'는 것 이다.  
즉, 자주 등장하거나 변화를 많이한 변수들의 경우 optimum에 가까이 있을 확률이 높기 때문에 작은 크기로 이동하면서 세밀한 값을 조정하고, 적게 변화한 변수들은 optimum값에 도달하기 위해서는 많이 이동해야 하므로 빠르게 loss값을 줄이는 방향으로 이동하는 방식이다.  
<div><img src="https://t1.daumcdn.net/cfile/tistory/99A5C94C5B629B7A0A" height="200" width="600" />
</div>
그림출처:<a href="https://nittaku.tistory.com/271">nittaku 블로그</a><br>
위의 그림을 위한 식안 아래에 자세히 나와있다.  
<p>$$G_t = G_{t-1} + (\nabla_{\theta}J(\theta_t))^{2}$$</p>
<p>$$\theta_{t+1} =\theta_{t} - \frac{\alpha}{\sqrt{G_t + \beta}} \bullet  \nabla_{\theta}J(\theta_t)$$</p>
<span>$$G_t$$</span>는 time step t까지 각 변수가 이동한 gradinet의 sum of squeares를 저장한다.  
계속해서 값을 누적하는 형태이므로 나누어주는 수가 결국에는 커져 w업데이트가 너무 느려진다.  
<span>$$\alpha$$ </span>는 <span>$$G_t$$</span>루트값에 반비례한 크기로 이동을 진행하여, 지금까지 많이 변화한 변수일 수록 적게 이동, 지금까지 많이 이동한 변수일수록 적게 이동을 하게 곱해주게 된다.  
즉, 모든 Weight들은 업데이트량이 비슷해지는 효과가 발생하게 된다.  
<span>$$\beta$$ </span>는 <span>$$10^{-4} ~ 10^{-8}$$</span>정도의 작은 값으로서 0으로 나누는 것을 방지하기 위한 작은 값이다.  

<br>
**RMS Prop**  
Adagrad의 단점을 해결하기 위한 방법이다.  
<span>$$G_t$$</span>부분을 합이 아니라 지수평균으로 바꾸어서 대처한 방법이다.  
이렇게 대체를 할 경우 Adagrad처럼 <span>$$G_t$$</span>가 무한정 커지지는 않으면서 최근 변화량의 변수간 상대적인 크기 차이는 유지할 수 있다.  
<div><img src="https://t1.daumcdn.net/cfile/tistory/99BE71425B629B7A09" height="200" width="600" />
</div>
그림출처:<a href="https://nittaku.tistory.com/271">nittaku 블로그</a><br>
위의 그림을 위한 식안 아래에 자세히 나와있다.  
<p>$$G = \alpha G + (1 - \alpha)(\nabla_{\theta}J(\theta_t))^{2}$$</p>
<p>$$\theta =\theta - \frac{\alpha}{\sqrt{G + \beta}} \bullet  \nabla_{\theta}J(\theta_t)$$</p>

<br>
**Adam**  
Adam (Adaptive Moment Estimation)은 RMSProp과 Momentum 방식을 합친 것 같은 알고리즘이다. 이 방식에서는 Momentum 방식과 유사하게 지금까지 계산해온 기울기의 지수평균을 저장하며, RMSProp과 유사하게 기울기의 제곱값의 지수평균을 저장한다.  

<div><img src="https://t1.daumcdn.net/cfile/tistory/997ADD3F5B629B7B04" height="200" width="600" />
</div>
그림출처:<a href="https://nittaku.tistory.com/271">nittaku 블로그</a><br>
위의 그림을 위한 식안 아래에 자세히 나와있다.  
<p>$$m_t = \beta_1 m_{t-1} + (1 - \beta_1)\nabla_{\theta}J(\theta)$$</p>
<p>$$v_t = \beta_2 v_{t-1} + (1 - \beta_2)(\nabla_{\theta}J(\theta))^{2}$$</p>

다만 m과 v가 처음에 0으로 초기화되어있기 때문에 초기 w업데이트 속도가 느리다는 단점이 생기게 된다.  




<hr>
참조: <a href="https://www.youtube.com/watch?v=cyPwxarw3XY&list=PL1H8jIvbSo1q6PIzsWQeCLinUj_oPkLjc&index=2">Chanwoo Timothy Lee Youtube</a> <br>
참조: <a href="https://darkpgmr.tistory.com/148">다크프로그래머 블로그</a><br>
참조: <a href="https://nittaku.tistory.com/271">nittaku 블로그</a><br>
참조: <a href="http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html">shuuki4 블로그</a><br>
문제가 있거나 궁금한 점이 있으면 wjddyd66@naver.com으로  Mail을 남겨주세요.