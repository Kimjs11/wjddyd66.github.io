---
layout: post
title:  "Optimization - Local Minima, Plateau, ZigZag"
date:   2019-07-14 10:00:00 +0700
categories: [AI]
---

### Local Minima
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

Local minima 문제는 에러를 최소화시키는 최적의 파라미터를 찾는 문제에 있어서 아래 그림처럼 파라미터 공간에 수많은 지역적인 홀(hole)들이 존재하여 이러한 local minima에 빠질 경우 전역적인 해(global minimum)를 찾기 힘들게 되는 문제를 일컫는다.  

<div><img src="https://t1.daumcdn.net/cfile/tistory/9965444D5B627B4412" height="200" width="600" />
</div>
그림출처:<a href="https://nittaku.tistory.com/271">nittaku 블로그</a><br>
**실제 딥러닝 모델에서는 Weight가 수도없이 많으며, 그 수많은 Weight가 모두 Local minima에 빠져야 Weight Update가 정지되기 때문에 ** 불가능하다. Local Minima을 해결하기 위하여 Optimization을 할 이유는 없다.  

### Plateau
Gradient Descent를 타고 Global Optima를 향해서 나아가는데, 평지(Plateau)가 생겨 loss가 업데이트 되지 않는 현상이 발생한다. 이러한 것을 Plateau현상 이라고 한다. 또한 Local Minima에 비해 일어날 확률이 매우 높다.  
<div><img src="https://t1.daumcdn.net/cfile/tistory/9933BB4C5B627B4514" height="200" width="600" />
</div>
그림출처:<a href="https://nittaku.tistory.com/271">nittaku 블로그</a><br>

### ZigZag현상
Weight를 Update 시키기 위한 BackPropagation을 Chain Rule에 적용시킨 결론은 아래와 같았다.  
<p>$$\delta(n-1) = \delta ng\prime(x) W$$</p>
<a href="https://wjddyd66.github.io/ai/2019/07/13/A.I-Backpropagation.html">BackPropagation 자세한 내용</a>  
**Active Function을 Sigmoid나 Relu**를 사용하게 되면, <span>$$\delta n$$</span>(output: 0~1) 및 <span>$$g\prime(x)$$</span>(Sigmoid의 편미분)이 모두 양수이므로 Weight업데이트량은 언제나 + or -가 나오며, 업데이트 방향을 잡을 때, 비효율적으로 ZigZag현상이 발생하여, 업데이트 현상이 느려진다.  

우리가 지금까지 사용해온<span style ="color: red">**Gradient Descent 로서는**</span>  
1. Local Minima
2. Plateau
3. ZigZag현상

위의 3개를 해결할 수 없다.  

### Optimazation
1. Gradient Descent
매 번 모든 데이터들에 대한 전부 살펴보고 기울기를 계산하므로 시간이 매우 많이 소요되는 문제가 있다.  
 - 뭉뚱한 부분에서 느림
 - Local Minima 현상 
 - Plateau 현상
 - ZigZag 현상

2. Momentum
Local Minima에 덜 빠지기 위해 Learning Rate에게 일종의 관성이라 할 수 있는 Momentum을 둔다. 직전에 나온 방향성 즉, 직전에 계산된 기울기를 고려하여 새로 계산된 기울기와 일정한 비율로 계산을 하는 것이다. 이렇게 하면 기울기가 갑자기 양수에서 음수로, 음수에서 양수로 바뀌는 경우가 줄어 들게 되고, 완만한 경사를 더 쉽게 타고 넘을 수 있게 된다.  
하지만 **ZigZag 현상**을 완벽히 해결하지는 못한다.  
<div><img src="https://t1.daumcdn.net/cfile/tistory/9929D1405B629B7635" height="200" width="600" />
</div>
그림출처:<a href="https://nittaku.tistory.com/271">nittaku 블로그</a><br>
아래 수식으로서 간단히 표현할 수 있다.  
<p>$$v \leftarrow \alpha v -  \beta \frac{\prime L}{\prime \theta}$$</p>
<p>$$\theta \leftarrow \theta + v$$</p>
새로운 하이퍼 파라미터인 <span>$$\alpha , v$$</span>가 새롭게 추가되 미분값이 계속하여 v에 더해져서 더욱 큰 값을 갖게되어 Plateau나 뭉뚱한 부분에서느림, Local Minima의 3가지를 해결할 수 있다.  

1. Gradient Descent
매 번 모든 데이터들에 대한 전부 살펴보고 기울기를 계산하므로 시간이 매우 많이 소요되는 문제가 있다.  
 - 뭉뚱한 부분에서 느림
 - Local Minima 현상 
 - Plateau 현상
 - ZigZag 현상

1. Gradient Descent
매 번 모든 데이터들에 대한 전부 살펴보고 기울기를 계산하므로 시간이 매우 많이 소요되는 문제가 있다.  
 - 뭉뚱한 부분에서 느림
 - Local Minima 현상 
 - Plateau 현상
 - ZigZag 현상

1. Gradient Descent
매 번 모든 데이터들에 대한 전부 살펴보고 기울기를 계산하므로 시간이 매우 많이 소요되는 문제가 있다.  
 - 뭉뚱한 부분에서 느림
 - Local Minima 현상 
 - Plateau 현상
 - ZigZag 현상


<hr>
참조: <a href="https://www.youtube.com/watch?v=cyPwxarw3XY&list=PL1H8jIvbSo1q6PIzsWQeCLinUj_oPkLjc&index=2">Chanwoo Timothy Lee Youtube</a> <br>
참조: <a href="https://darkpgmr.tistory.com/148">다크프로그래머 블로그</a><br>
참조: <a href="https://nittaku.tistory.com/271">nittaku 블로그</a><br>
문제가 있거나 궁금한 점이 있으면 wjddyd66@naver.com으로  Mail을 남겨주세요.