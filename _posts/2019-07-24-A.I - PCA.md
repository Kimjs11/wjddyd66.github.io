---
layout: post
title:  "PCA"
date:   2019-07-24 10:00:00 +0700
categories: [AI]
---

### PCA
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
PCA(Principal Component Analysis)은 **차원축소(dimensionality reduction)**와 **변수추출(feature extraction)**기법으로 널리쓰이고 있다.  
기본적인 개념을 위해 아래 그림을 보자  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/24.PNG" height="250" width="600" /></div>
**그림1**  
위와같인 점들이 분포하고 있고, 점의 개수가 5만개라고 가정하면  
Model을 만들기 위해서는 x,y의 값을 각각 5만개씩 **총 10만개**의 값을 알아야 한다.  
하지만 이러한 점의 분포가 아래와 같다 생각하면  

<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/25.PNG" height="250" width="600" /></div>
**그림2**  
점의 개수가 5만개여도 y=ax+b의 식으로 나타낼 수 있으며  
**x or y의 값 (5만개) + a , b의 값 (2개) = 50002개**의 값만 알면 되므로 Model을 만들기 위해 알아야 하는 값이 반으로 줄어드는 효과를 얻을 수 있다.  

**그림1**은 Linear Regression **그림2**은 PCA를 통하여 비교하여 보자.  
Cost Function을 통하여 Model을 만든다고 가정하면  
**그림1**은 **예측값과 실제값의 차이**를 통하여 Cost가 최소가 되는 Model을 만들게 된다.  

<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/26.PNG" height="250" width="600" /></div>  
**그림2**은 **예측값과 실제값의 거리**를 통하여 Cost가 최소가 되는 Model을 만들게 된다.  
<a href="https://wjddyd66.github.io/ai/2019/07/08/A.I-Linear-Regression.html">Linear Regression 자세한 내용</a>  

<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/27.PNG" height="250" width="600" /></div>  
<span style ="color: red">**PCA는 데이터의 분산(variance)을 최대한 보존하면서 서로 직교하는 새 기저(축)를 찾아, 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간으로 변환하는 기법입니다. **</span>  
아래 그림은 이러한 PCA를 잘 나타내는 그림이다.  

<div><img src="http://i.imgur.com/Uv2dlsH.gif" height="100%" width="100%" /></div>  

그림 출처:<a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues">stats.stackexchange.com</a>  

PCA를 잘 알기위한 알아야 하는 고유값 분해, SVD에 대해 먼저 알아보자  

### 고유값 분해
**고유값, 고유벡터**  
고유값 분해(Eigen Decomposition)를 알기 위해서 먼저 고유값(Eigenvalue), 고유백터(Eigenvector)가 무엇인지 알아야 한다.  
<p>$$Av = \lamda v$$</p>
위의 식에서 <span>$$\lamda$$ </span>는 행렬 A의 고유값, v는 행렬 A의 고유벡터라고 불리게 된다.  
A의 고유벡터는 A에 의해 **방향**은 보존되고 **크기**만 커진다고 말할 수 있다.  
<span style ="color: red">**선형변환 A에 의한 변환 결과가 자기 자신의 상수배가 되는 0이 아닌 벡터를 고유벡터(eigenvector)라 하고 이 상수배 값을 고유값(eigenvalue)**</span>이다.  

<br>
**고유값분해를 이용한 대각화**  
아래와 같은 행렬이 존재한다고 가정하자.  
<p>$$\begin{bmatrix} V_11 & ... & V_1n \\  ... & ... & ... \\ V_n1 & ... & V_nn  \end{bmatrix} \begin{bmatrix} \lamda_1 &  ... & 0 \\ 0 &  \lamda_2 & ... \\ 0 &  ... & \lamda_n \end{bmatrix} = \begin{bmatrix} \lamda_1v_11 & ... & \lamda_nv_1n \\ \lamda_1v_21 & ... & \lamda_nv_2n \\ \lamda_1v_n1 & ... & \lamda_nv_nn  \end{bmatrix}$$</p>

행렬 A의 고유값, 고유벡터들을 <span>$$\lamda_i, v_i  (i=1,2, ... , n)$$ </span>이라 가정하면 아래와 같이 나타낼 수 있다.  

### SVD 
SVD(Singular Value Decomposition) 특이값 분해는 고유값 분해 처럼 행렬을 대각화하는 한 방법이다.  
SVD의 중요한 점은 **정방행렬이든 아니든 관계없이 모든 m x n 행렬에 대해 적용이 가능**하기 때문이다.  
고유값 분해는 정방행렬에 대해서만 적용가능하며 또한 정방행렬중에서도 일부 행렬에 대해서만 적용 가능한 대각화 방법이다.  

<p>$$x(t) \ast h(t) = \int_{- \infty}^\infty x( \tau)t(x - \tau)d\tau$$</p>
<div><img  src="https://upload.wikimedia.org/wikipedia/commons/b/b9/Convolution_of_spiky_function_with_box2.gif" width="500" height="300"></div>
그림출처:<a href="https://en.wikipedia.org/wiki/Convolution">위키피디아</a>  
이러한 Convolution은 추력값이 어떠한 한 시점에만 영향을 받는 것이 아니라, 이전의 입력 값들에도 영향을 받기 때문에 단순한 곱셈이 아닌 합성곱 형태로 나타내는 것이다.  
<span style ="color: red">**또한 결과값이 클 수록 서로 같은 성분을 많이 가지고 있는것을 위의 그림에서 알 수 있다.**</span>  

<hr>
참조: <a href="https://www.youtube.com/watch?v=85SdLVOu3GQ&list=PL1H8jIvbSo1q6PIzsWQeCLinUj_oPkLjc&index=9">Chanwoo Timothy Lee Youtube</a> <br>
참조: <a href="https://ratsgo.github.io/machine%20learning/2017/04/24/PCA">ratsgo's blog</a> <br>
문제가 있거나 궁금한 점이 있으면 wjddyd66@naver.com으로  Mail을 남겨주세요.