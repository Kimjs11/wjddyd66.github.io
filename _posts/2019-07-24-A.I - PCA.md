---
layout: post
title:  "PCA"
date:   2019-07-24 10:00:00 +0700
categories: [AI]
---

### PCA
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
PCA(Principal Component Analysis)은 **차원축소(dimensionality reduction)**와 **변수추출(feature extraction)**기법으로 널리쓰이고 있다.  
기본적인 개념을 위해 아래 그림을 보자  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/24.PNG" height="250" width="600" /></div>
**그림1**  
<br><br>
위와같인 점들이 분포하고 있고, 점의 개수가 5만개라고 가정하면  
Model을 만들기 위해서는 x,y의 값을 각각 5만개씩 **총 10만개**의 값을 알아야 한다.  
하지만 이러한 점의 분포가 아래와 같다 생각하면  

<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/25.PNG" height="250" width="600" /></div>
**그림2**<br><br>  
점의 개수가 5만개여도 y=ax+b의 식으로 나타낼 수 있으며  
**x or y의 값 (5만개) + a , b의 값 (2개) = 50002개**의 값만 알면 되므로 Model을 만들기 위해 알아야 하는 값이 반으로 줄어드는 효과를 얻을 수 있다.  

**그림1**은 Linear Regression **그림2**은 PCA를 통하여 비교하여 보자.  
Cost Function을 통하여 Model을 만든다고 가정하면  
**그림1**은 **예측값과 실제값의 차이**를 통하여 Cost가 최소가 되는 Model을 만들게 된다.  

<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/26.PNG" height="250" width="600" /></div>  
**그림2**은 **예측값과 실제값의 거리**를 통하여 Cost가 최소가 되는 Model을 만들게 된다.  
<a href="https://wjddyd66.github.io/ai/2019/07/08/A.I-Linear-Regression.html">Linear Regression 자세한 내용</a>  

<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/27.PNG" height="250" width="600" /></div>  
<span style ="color: red">**PCA는 데이터의 분산(variance)을 최대한 보존하면서 서로 직교하는 새 기저(축)를 찾아, 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간으로 변환하는 기법이다.**</span>  
아래 그림은 이러한 PCA를 잘 나타내는 그림이다.  

<div><img src="http://i.imgur.com/Uv2dlsH.gif" height="100%" width="100%" /></div>  

그림 출처:<a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues">stats.stackexchange.com</a>  

PCA를 잘 알기위한 알아야 하는 고유값 분해, SVD에 대해 먼저 알아보자  

### 고유값 분해
**고유값, 고유벡터**  
고유값 분해(Eigen Decomposition)를 알기 위해서 먼저 고유값(Eigenvalue), 고유백터(Eigenvector)가 무엇인지 알아야 한다.  
<p>$$Av = \lambda v$$</p>
위의 식에서 <span>$$\lambda$$ </span>는 행렬 A의 고유값, v는 행렬 A의 고유벡터라고 불리게 된다.  
A의 고유벡터는 A에 의해 **방향**은 보존되고 **크기**만 커진다고 말할 수 있다.  
<span style ="color: red">**선형변환 A에 의한 변환 결과가 자기 자신의 상수배가 되는 0이 아닌 벡터를 고유벡터(eigenvector)라 하고 이 상수배 값을 고유값(eigenvalue)**</span>이다.  

<br>
**고유값분해를 이용한 대각화**  
아래와 같은 행렬이 존재한다고 가정하자.  
<p>$$\begin{bmatrix} V_11 & ... & V_1n \\  ... & ... & ... \\ V_n1 & ... & V_nn  \end{bmatrix} \begin{bmatrix} \lambda_1 &  ... & 0 \\ 0 &  \lambda_2 & ... \\ 0 &  ... & \lambda_n \end{bmatrix} = \begin{bmatrix} \lambda_1v_11 & ... & \lambda_nv_1n \\ \lambda_1v_21 & ... & \lambda_nv_2n \\ \lambda_1v_n1 & ... & \lambda_nv_nn  \end{bmatrix}$$</p>

행렬 A의 고유값, 고유벡터들을 <span>$$\lambda_i, v_i  (i=1,2, ... , n)$$ </span>이라 가정하면 아래와 같이 나타낼 수 있다.  
<p>$$Av_1 = \lambda_1v_1$$ </p>
<p>$$Av_2 = \lambda_2v_2$$ </p>
<p>$$ ... $$ </p>
<p>$$Av_n = \lambda_1v_n$$ </p>
위의 식을 한꺼번에 정리하게 되면 아래와 같은 식으로 나타낼 수 있다.  
<p>$$A\begin{bmatrix} v_1 & v_2 & ... & v_n  \end{bmatrix} = \begin{bmatrix} \lambda_1v_1 & \lambda_2v_2 & ... & \lambda_nv_n  \end{bmatrix} = \begin{bmatrix} v_1 & v_2 & ... & v_n  \end{bmatrix}\begin{bmatrix} \lambda_1 & 0 & ... & 0 \\ 0 & \lambda_2 & ... & 0 \\ ... \\ 0 & 0 & ... & \lambda_n   \end{bmatrix}$$ </p>
위에서 고유값들을 대각원소로하는 행렬을 대각행렬 <span>$$ \land $$ </span>로 정의하게 되면 아래와 같은 식으로 간단하게 표현 될 수 있다.  
<p>$$ AP = P \land $$</p>
<p>$$ A = P \land P^{-1}$$</p>
이와 같이 **행렬A**는 **자신의 고유벡터들을 열벡터로 하는 행렬**과 **고유값을 대각원소로하는 행렬의 곱**으로 분해가 가능한데 이러한 것을 **대각화 분해(eigendecomposition)**라고 한다.  

이러한 대각화 분해로 인한 det(A), A의 거듭제곱, 역행렬, 대각합(trace), 다항식 등 매우 손쉽게 계산할 수 있다.  
아래의 예시는 A의 거듭제곱을 나타낸 것 이다.  
<p>$$ A^{k} = (P \land P^{-1})^{k}$$</p>
<p>$$ = (P \land P^{-1})(P \land P^{-1}) ... (P \land P^{-1})$$</p>
<p>$$ P  \land^{k} P^{-1}$$</p>
<p>$$ P diag(\lambda_1^{k},....,\lambda_n^{k}) P^{-1}$$</p>
<br>
**고유값, 고유벡터 계산**  
고유값과 고유벡터를 구하는 방법에 대해 알아보자.  
<p>$$Av = \lambda v$$</p>
<p>$$Av - \lambda v = 0 (0: 0행렬)$$</p>
<p>$$(A - \lambda E) v = 0 (E: 단위행렬)$$</p>
위의 식에서 <span>$$ v \neq 0 $$</span>이므로 <span>$$ A - \lambda E $$</span>의 역행렬이 존재하면 안된다.  
위의 조건을 만족하기 위한 식은 아래와 같다.  
<p>$$det(A - \lambda E) v = 0$$</p>

<span style ="color: red"> **자세한 고유값 분해에 대한 예시와 자세한 내용은 링크 참조:** </span> <a href="https://darkpgmr.tistory.com/105">다크프로그래머 blog</a>  

### SVD 
SVD(Singular Value Decomposition) 특이값 분해는 고유값 분해 처럼 행렬을 대각화하는 한 방법이다.  
SVD의 중요한 점은 **정방행렬이든 아니든 관계없이 모든 m x n 행렬에 대해 적용이 가능**하기 때문(고유값 분해는 정방행렬일때만 가능)이다.   



<hr>
참조: <a href="https://www.youtube.com/watch?v=85SdLVOu3GQ&list=PL1H8jIvbSo1q6PIzsWQeCLinUj_oPkLjc&index=9">Chanwoo Timothy Lee Youtube</a> <br>
참조: <a href="https://ratsgo.github.io/machine%20learning/2017/04/24/PCA">ratsgo's blog</a> <br>
참조: <a href="https://darkpgmr.tistory.com/105">다크프로그래머 blog</a> <br>
문제가 있거나 궁금한 점이 있으면 wjddyd66@naver.com으로  Mail을 남겨주세요.