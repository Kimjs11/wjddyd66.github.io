---
layout: post
title:  "KoNLPy"
date:   2019-08-10 09:00:00 +0700
categories: [DL]
---

### KoNLPy
**NLP (Natural Language Processing, 자연어처리)**는 **텍스트에서 의미있는 정보를 분석, 추출하고 이해하는 일련의 기술집합**입니다.  
**KoNLPy는 여러분이 한국어 텍스트를 이용하여 기초적인 NLP 작업을 수행하는데 도움을 드릴 것**입니다.  

### 현태소 분석 및 품사 태깅
**형태소 분석** 이란 형태소를 비롯하여, 어근, 접두사/접미사, 품사(POS, part-of-speech) 등 다양한 언어적 속성의 구조를 파악하는 것입니다.  
**품사 태깅** 은 형태소의 뜻과 문맥을 고려하여 그것에 마크업을 하는 일입니다.  
다음은 형태소 분석 중에서 Kkma를 사용했을 경우이다.  
```python
kkma = Kkma()
pprint(kkma.sentences("여러분, 안녕! 반갑습니다.")) 
print(kkma.nouns("여러분, 안녕! 반갑습니다."))
print(kkma.pos("여러분, 안녕! 반갑습니다."))
```
<br>
```code
['여러분, 안녕! 반갑습니다.']
['여러분', '안녕']
[('여러분', 'NP'), (',', 'SP'), ('안녕', 'NNG'), ('!', 'SF'), ('반갑', 'VV'), ('습니다', 'EFN'), ('.', 'SF')]
```
<br>
KoNLPy에는 Kkma외에 Okt, Komoran, Hannanum등이 존재 한다.  
아래는 Okt를 사용했을 경우의 예시이다.  
<br>
```python
from konlpy.tag import Okt
okt = Okt()
print(okt.nouns("여러분, 안녕! 반갑습니다."))
print(okt.pos("여러분, 안녕! 반갑습니다."))
```
<br>
```code
['여러분', '안녕']
[('여러분', 'Noun'), (',', 'Punctuation'), ('안녕', 'Noun'), ('!', 'Punctuation'), ('반갑습니다', 'Adjective'), ('.', 'Punctuation')]
```
<br>
형태소 분석에서 형태소만을 추출하는 함수가 있다.  
1. nouns(): 명사만을 추출하는 함수
2. morphs(): 모든 품사를 추출하는 함수
3. pos(): 단어 하나하나 품사를 명시해주는 함수


<br>
```python
twitter = Okt()
wordlist = twitter.nouns("멋진 봄은 엄청 무더운 여름과 한들한들 시원한 바람이 부는 가을의 중간 계절이다")
print(wordlist)
wordlist2 = twitter.morphs("멋진 봄은 엄청 무더운 여름과 한들한들 시원한 바람이 부는 가을의 중간 계절이다")
print(wordlist2)
wordlist3 = twitter.pos("멋진 봄은 엄청 무더운 여름과 한들한들 시원한 바람이 부는 가을의 중간 계절이다")
print(wordlist3)
```
<br>
```code
['봄', '여름', '바람', '가을', '중간', '계절']
['멋진', '봄', '은', '엄청', '무더운', '여름', '과', '한들한들', '시원한', '바람', '이', '부는', '가을', '의', '중간', '계절', '이다']
[('멋진', 'Adjective'), ('봄', 'Noun'), ('은', 'Josa'), ('엄청', 'Adverb'), ('무더운', 'Adjective'), ('여름', 'Noun'), ('과', 'Josa'), ('한들한들', 'Adverb'), ('시원한', 'Adjective'), ('바람', 'Noun'), ('이', 'Josa'), ('부는', 'Verb'), ('가을', 'Noun'), ('의', 'Josa'), ('중간', 'Noun'), ('계절', 'Noun'), ('이다', 'Josa')]
```
<br>
다음 예제는 **웹스크래핑 후 형태소를 분석**하는 예제이다.  
웹스크래핑을한 URL:<a href="https://ko.wikipedia.org/wiki/%EC%9D%B4%EC%88%9C%EC%8B%A0">위키백과</a>  
<br>
```python
import urllib.request
from bs4 import BeautifulSoup
from konlpy.tag import *
from urllib import parse

kkma = Kkma()
twitter = Okt()
hana = Hannanum()

sdata = parse.quote("이순신")
url = "https://ko.wikipedia.org/wiki/" + sdata
page = urllib.request.urlopen(url)
soup = BeautifulSoup(page.read(), "html.parser")
#print(soup)

wordlist = []

for item in soup.select("#mw-content-text > div > p"):
    if item.string != None:
        print(item.string)
        ss = item.string
        wordlist +=twitter.nouns(ss)
        
print(wordlist)        
print("단어 수: "+str(len(wordlist)))
```
<br>
```code
당시 조산만호 이순신은 북방 여진족의 약탈 및 침략을 예상하고 수비를 강화하기 위하여 여러차례 북병사

....

'이순신', '대한', '평가', '이순신', '일기', '시조', '한시', '등', '여러', '편의', '작품']
단어 수: 253
```
<br>
위의 결과의 단어 발생 횟수를 확인하는 예제이다.  
<br>
```python
word_dict = {}
for i in wordlist:
    if i in word_dict:
        word_dict[i] += 1
    else: 
        word_dict[i] = 1
        
print(word_dict)
```
<br>
```code
{'당시': 3, '조산': 1, '만호': 1, '이순신': 16, '북방': 1, '여진족': 1, '약탈': 1, '및': 2, '침략': 1, '예상': 1, '수비': 1, '위': 1, '차례': 1, '북': 

...

'완전무결': 1, '성자': 1, '영웅': 1, '천관우': 1, '등': 2, '일기': 1, '시조': 1, '한시': 1, '여러': 1, '편의': 1, '작품': 1}
```
<br>
중복 단어를 제거하는 코드이다.  
<br>
```python
setdata = set(wordlist)    
print("발견 된 명사의 갯수(중복제거): " + str(len(setdata)))
```
<br>
```code
발견 된 명사의 갯수(중복제거): 180
```
<br>
위의 결과를 **DataFrame**으로서 바꾸는 예제이다.  
<br>
```python
print("DataFrame으로 읽어들이기")
from pandas import Series, DataFrame
li_data = Series(wordlist)
print(li_data)
seri_data = Series(word_dict)
print(seri_data)
print(seri_data.value_counts()[:5])

print()
df = DataFrame(wordlist)
print(type(df))
```
<br>
```code
 DataFrame으로 읽어들이기
0       당시
1       조산
2       만호
3      이순신
4       북방
      ... 
248     한시
249      등
250     여러
251     편의
252     작품
Length: 253, dtype: object
당시      3
조산      1
만호      1
이순신    16
북방      1
       ..
시조      1
한시      1
여러      1
편의      1
작품      1
Length: 180, dtype: int64
1    143
2     25
3      6
6      2
5      2
dtype: int64

<class 'pandas.core.frame.DataFrame'>
```
<br>
위의 결과를 Plot을 활용하여 시각화하여 보여주는 예제이다.  
<br>
```python
import matplotlib.pyplot as plt
plt.rc("font", family="malgun gothic") #한글깨짐 방지
plt.plot(seri_data.value_counts()[:5])
plt.xlabel("횟수 종류")
plt.ylabel("종류별 발생 수")
plt.title("단어 건수")
plt.legend("횟수")
plt.show() #최종적으로 plot을 보이게 하는 명령
```
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/38.PNG" height="250" width="600" /></div>
<br>
### Word2vec
Word2vec이란 **비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다**라는 가정으로서 단어를 표현하는 것 입니다.  
**분포 가설**에 따라서 텍스트를 벡터화한다면 단어들이 의미적으로 가까운 단어는 거리가 짧게 나타나고 단어들이 의미적으로 먼 단어는 거리가 멀게 나타납니다.  
이렇게 표현된 벡터들은 벡터의 차원이 단어집합의 크기일 필요도 없고 **벡터의 차원이 저차원으로 줄어든다는 장점**이 있다.  
Word2vec을 활용한 예제는 아래와 같다.  
1. 고양이 + 애교 = 강아지
2. 한국 - 서울 + 도쿄 = 일본
3. 박찬호 - 야구 + 축구 = 호나우두

Word2vec에 대한 자세한 내용:<a href="https://wikidocs.net/22660">wikidocs blog</a>  
<br>
아래 예시는 명사구, 동사구, 형용사구로 묶어주는 예시이다.  
<br>
```python
import nltk
parser_ko = nltk.RegexpParser("NP:{<Adjective>*<Noun>*}")
chunk_ko = parser_ko.parse(wordlist3)
print(chunk_ko)
```
<br>
```code
(S
  (NP 멋진/Adjective 봄/Noun)
  은/Josa
  엄청/Adverb
  (NP 무더운/Adjective 여름/Noun)
  과/Josa
  한들한들/Adverb
  (NP 시원한/Adjective 바람/Noun)
  이/Josa
  부는/Verb
  (NP 가을/Noun)
  의/Josa
  (NP 중간/Noun 계절/Noun)
  이다/Josa)
```
<br>
### corpus(말뭉치)
말뭉치라는 것은 언어의 표본을 담아둔 묶음(단어사전)이다.  
NLP에서 나중에 이러한 말뭉치를 작성을 하게 된다면 **단어의 빈도수**에 따라서 **단어가 나올 확률을 높여주는 가중치의 역할**을 할 수 있다.  
<a href="https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=105&oid=005&aid=0001223252">뉴스 기사</a>를 말뭉치로서 나눠본 것이다.  

<br>
```python
from konlpy.corpus import kobill

files_ko = kobill.fileids()
#print(files_ko)
doc_ko = kobill.open('abc2.txt').read()
#print(doc_ko)

#의미 단어 추출 (Tokenize)
from konlpy.tag import Okt
t = Okt()
tokens_ko = t.morphs(doc_ko) # 문서를 token으로 분리
print(tokens_ko)

import nltk
ko = nltk.Text(tokens_ko, name='번역')
print(ko)

print('토큰 정보 확인 ')
print(len(ko.tokens))
print(len(set(ko.tokens)))
fre_dist = ko.vocab()
print(fre_dist) # <FreqDist with 294 samples and 618 outcomes>
```
<br>
```code
['\ufeff', '이통', '사들이', 'IT', '업체', '들', '과', '손잡고', '번역', '서비스', '를', '잇', '달', '아', '제공', '하고', '있다', '.', '인공', '지능', '(', 'AI', ')', '반의', '번역', '서비스', '가', '등장', '하면서', '번역', '에', '대한', '수요', '가', '늘어나

...

<Text: 번역>
토큰 정보 확인 
224
128
<FreqDist with 128 samples and 224 outcomes>
```
<br>
아래 코드는 **웹 데이터를 읽을 시 Multiprocessing**으로서 처리하는 예제이다.  
먼저 Multiprocessing으로 실행 할 시 얼마나 시간이 줄어드는지 확인하기 위하여 **Singleprocessing**으로 실행하여 보였다.  
<br>
```python
#멀티프로세싱 없이 웹 데이터 읽기
import requests
from bs4 import BeautifulSoup
import time

def get_alinks():
    req_data = requests.get("https://beomi.github.io/beomi.github.io_old/")
    html = req_data.text
    #print(html)
    
    soup = BeautifulSoup(html, "html.parser")
    a_titles = soup.select("h3 > a")
    data = []
    for t in a_titles:
        data.append(t.get("href"))
    
    return data    

def get_cont(link):
    a_link = "https://beomi.github.io" + link
    req = requests.get(a_link)
    html = req.text
    soup = BeautifulSoup(html, "lxml")
    print(soup.select("h1")[0].text)
    
if __name__ == "__main__":
    start_time = time.time()
    print(get_alinks())
    for link in get_alinks():
        get_cont(link)
        
    print("소요시간: %s초"%(time.time() - start_time))
```
<br>
```code
['/beomi.github.io_old/python/2017/02/28/HowToMakeWebCrawler-Save-with-Django.html', '/beomi.github.io_old/python/2017/02/27/HowToMakeWebCrawler-With-Selenium.html', '/beomi.github.io_old/python/2017/02/08/Setup-SocialAuth-for-Django.html', '/beomi.github.io_old/python/2017/02/01/Django-CustomAuth.html', '/beomi.github.io_old/python/

...

pip로 mysqlclient설치 중 mac os x에서 egg_info / OSError 발생시 대처방법
소요시간: 14.381848812103271초
```
<br>
아래 코드는 Multiprocessing으로서 확인하기 위한 예시이다.  
<span style ="color: red">**하지만 실제 환경인 Window에서 Jupyter Notebook으로는 MultiProcessing을 지원하지 않는 경향을 보이고 있다.**</span>  
따라서 결과의 사진은 같이 실습을 하였던 다른 사람의 결과를 가져왔다.  

<br>

```python
from multiprocessing import Pool
start_time = time.time()
pool = Pool(processes=4) #process 4개 사용
pool.map(get_cont, get_alinks())
    
print("소요시간: %s초"%(time.time() - start_time))
```
<br>
```code
소요시간: 2.2359251976013184초
[출처] [190521] KoNLPy 모듈을 이용한 형태소 분석/ word2vec/ corpus(말뭉치), 워드클라우드/ 멀티프로세싱/ 파이썬에서 웹 서버 운영하기|작성자 천프로
```
<br>
<span style ="color: red">**Window에서 Jupyter Notebook으로는 MultiProcessing시 Error**</span>  
<div><img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/39.PNG" height="250" width="600" /></div>

<br>
<hr>
참조: <a href="https://github.com/wjddyd66/DeepLearning/blob/master/KoNLPy.ipynb">원본코드</a> <br>
코드에 문제가 있거나 궁금한 점이 있으면 wjddyd66@naver.com으로  Mail을 남겨주세요.