---
layout: post
title:  "Tensorflow-DQN"
date:   2019-12-01 09:00:00 +0700
categories: [Tensorflow]
---
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
### 강화학습
강화학습이란, **주어진 어떤 상황(State)에서 보상(Reward)을 최대화 할 수 있는 행동(Action)에 대해 학습하는 것**이다.  
위의 과정을 위해서 학습의 **주체(Agent)가 상황에 가장 적합한 행동을 찾기까지는 수많은 시행착오가 필요하다.**  
위의 과정을 사진으로 나타내면 아래와 같다.  
<img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/165.png" height="100%" width="100%" /><br>
사진 출처: <a href="https://brunch.co.kr/@kakao-it/73">Kakao 블로그</a><br>
위의 각각의 요소를 자세히 나누면 다음과 같다.  
- S: 상태(State)들의 집합
- A: 행동(Action)들의 집합
- R: 보상(Reward)들의 집합
- R(<span>$$s,s^{'}$$</span>): 행동 a에 의해서 상태 s에서 다음 상태 <span>$$s^{'}$$</span>로 넘어갈때 얻는 보상(Rewoard)
- P(<span>$$s,s^{'}$$</span>): 상태 s에서 다음 상태 <span>$$s^{'}$$</span>로 넘어갈 전이 확률(Transition Probability)
- <span>$$\gamma$$</span>: 현재 보상과 미래 보상의 중요도를 조정하는 Discount Factor


위의 예시로서 좋은 벽돌깨기 게임을 생각해보자.  
<img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/166.png" height="100%" width="100%" /><br>
위의 사진과 같은 Game에서 **Ageng(Bar)**가 할 수 있는 **Action**은 3가지이다.  
1) 움직이지 않기  
2) 좌로 움직이기  
3) 우로 움직이기  

다음과 같은 3가지의 Acion에 대해서 만약 **공이 많은 벽돌을 깨뜨리면 많은 보상(Reward)를 얻을 수 있고, 만약 죽게되면 적은 보상(Reward)를 얻을 수 있다.**  
따라서 각각의 Image에 따라서 Agent는 어떤 Action을 취해야지 더 많은 보상(Reward)를 얻을 수 있는지에 대해 계속해서 Training을 실시한다.  
이러한 Agent가 현재 상태에서 어떻게 행동할 것인지를 결정하는 전략을 <span>$$\pi$$</span>로 표현하고 최적의 정책은 <span>$$\pi^{*}$$</span>를 찾는 것을 학습의 목표로서 표현한다.  
이러한 <span>$$\pi^{*}$$</span>을 찾기 위해서는 **상태 가치 함수와 행동 가치 함수**라는 개념을 사용한다.  

<br><br>

### 상태 가치 함수, 행동 가치 함수

**상태 가치 함수**  
**상태 가치 함수란 현재 상태의 좋음과 나쁨을 표현한다.**  
상태 가치 함수는 아래 수식으로 표현된다.  
<p>$$V_{\pi}(s) = E_{\pi}[R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+ ... |S_{t}=s]$$</p>
위의 수식을 살펴보게 되면 위에서 어떠게 행동할 것인지 결정하는 전략 <span>$$\pi$$</span>에 대해서 현재 상태 (<span>$$S _ {t}$$</span>)에 대한 보상(<span>$$R_{t}, R_{t+1}, ...$$</span>)으로서 표현한다.  
위에서 <span>$$\gamma$$</span>는 현재 보상과 미래 보상의 중요도를 조정하는 Disount Factor로서 0 ~ 1사이의 값으로서 표현된다.  
즉, 현재 시점 (t)에서의 상태 가치 함수는 <span>$$\gamma \neq 0$$</span>이면 미래의 시간에 대한 보상까지 생각하여 계산한 값이지만, 통상적으로 Reward가 매우 크지 않으면 가까운 시간에 영향을 많이 받는 값으로서 표현된다는 거 이다.  
만약 미로 찾기에서 Goal을 찾아가는 문제로 나타내게 되면 아래 그림과 같이 표현할 수 있다.  
<img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/167.png" height="100%" width="100%" /><br>
사진 출처: <a href="https://blog.naver.com/PostView.nhn?blogId=horajjan&logNo=221426201763&parentCategoryNo=&categoryNo=122&viewDate=&isShowPopularPosts=true&from=search">일체유심조 Blog</a>  
각각이 그림은 <span>$$\gamma$$</span>의 상태에 따라 달라진다.  
또한 이해가 바로가는 책에서의 예제는 다음과 같이 나와있다.  
> 스타크래프트 게임을 플레이하는 상황을 가정해보면 초반에 일꾼을 이용해서 미니맵을 정찰하는 것은 당장에는 보상이 없지만 미래에 어떤 전력을 펼칠지 결저알 수 있도록 도와주는 중요한 정보를 얻을 수 있는 행동입니다.

<br>
**행동 가치 함수**  
**행동 가치 함수는 현재 행동의 좋음과 나쁨을 표현한다.**  
실질적인 강화학습에서는 Agent의 행동에 따른 보상을 제공한다.  
따라서 앞으로의 방식은 행동 가치 함수를 통하여 Training하고 결과를 확인하는 과정이 될 것이다.  
행동 가치 함수를 수식으로 살펴보면 다음과 같다.  
<p>$$Q_{\pi}(s,a) = E_{\pi}[R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+ ... |S_{t}=s,A_{t}=a]$$</p>
위의 수식을 살펴보게 되면 상태가치 함수와 식이 비슷하나 다른것은 **State 뿐만 아니라 Action에 대해서도 고려를 한다는 것** 이다.  
만약 미로 찾기에서 Goal을 찾아가는 문제로 나타내게 되면 아래 그림과 같이 표현할 수 있다.  
<img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/168.png" height="100%" width="100%" /><br>
사진 출처: <a href="https://blog.naver.com/PostView.nhn?blogId=horajjan&logNo=221426201763&parentCategoryNo=&categoryNo=122&viewDate=&isShowPopularPosts=true&from=search">일체유심조 Blog</a>  
참고로 상태 가치 함수를 이용해서 알아내는 방법을 Planning이라고 하고 행동 가치 함수를 이용해서 알아내는 방법을 강화 학습 이라고 한다.  
<br><br>

### Q-Learning
Q-Learning은 적절한 행동 가치 함수값을 알아내기 위한 알고리즘이다.  
수학적인 수식으로서 살펴보게 되면 식은 아래와 같다.  
<p>$$Q(s_t,a_t) = R_{t+1} + \gamma max_{a+1}Q(s_{t+1},a_{t+1})$$</p>
솔직히 처음 식을 보고 Update를 어떻게 시킬 것인지 혹은 각각의 행렬은 무슨 의미를 가지는지 매우 어렵다.  
따라서 좋은 예제인 <a href="https://m.blog.naver.com/PostView.nhn?blogId=infoefficien&logNo=220769665748&proxyReferer=https%3A%2F%2Fwww.google.com%2F">gaussian37</a>의 Blog의 예제를 살펴보자.  

#### Q-Table
먼저 아래와 같은 사진의 문제를 정의하자.  
<img src="https://mblogthumb-phinf.pstatic.net/20160822_279/infoefficien_1471857210056fyKks_JPEG/%C6%F7%B8%CB%BA%AF%C8%AF_1.jpg?type=w800"/><br>
사진 출처: <a href="https://m.blog.naver.com/PostView.nhn?blogId=infoefficien&logNo=220769665748&proxyReferer=https%3A%2F%2Fwww.google.com%2F">gaussian37</a><br>
위와 같이 5개의 방이 있다고 가정한다. 5번방이 Goal이라고 생각하면 위의 사진은 아래와 같이 node와 edge로서 표현할 수 있다.  
<img src="https://mblogthumb-phinf.pstatic.net/20160822_293/infoefficien_1471857224998pEtY9_JPEG/%C6%F7%B8%CB%BA%AF%C8%AF_2.jpg?type=w800"/><br>
사진 출처: <a href="https://m.blog.naver.com/PostView.nhn?blogId=infoefficien&logNo=220769665748&proxyReferer=https%3A%2F%2Fwww.google.com%2F">gaussian37</a><br><br>
Goal로 가는 Action의 값을 100이라 두고 다른 Action은 모두 0이라고 가정하면 아래 그림과 같이 나타낼 수 있다.  
<img src="https://mblogthumb-phinf.pstatic.net/20160822_76/infoefficien_1471857235025yoYMX_JPEG/%C6%F7%B8%CB%BA%AF%C8%AF_3.jpg?type=w800"/><br>
사진 출처: <a href="https://m.blog.naver.com/PostView.nhn?blogId=infoefficien&logNo=220769665748&proxyReferer=https%3A%2F%2Fwww.google.com%2F">gaussian37</a><br><br>
위의 Node와 Edge를 행렬을 다음의 조건에 적용시켜 행렬로 나타내보자.  
- 상태(State)는 현재 방의 위치이다.
- 행동(Action)은 다음 방으로 이동할 수 있는 경로 이다.
- 이동할 수 있으면 0, Goalㄹ 이동할 수 있으면 100, 이동할 수 없으면 -1로서 Action의 값을 할당한다.

<br>
<img src="https://mblogthumb-phinf.pstatic.net/20160822_94/infoefficien_1471857319337AH04p_JPEG/%C6%F7%B8%CB%BA%AF%C8%AF_6.jpg?type=w800"/><br>
사진 출처: <a href="https://m.blog.naver.com/PostView.nhn?blogId=infoefficien&logNo=220769665748&proxyReferer=https%3A%2F%2Fwww.google.com%2F">gaussian37</a><br><br>
위의 Matrix는 결과적으로 **Agent가 State에서 Action을 취함으로써 얻을 수 있는 Reward Matrix**가 된다.  
여기서 잠깐 Q-Learning 식을 다시한번 살펴보자.  
<p>$$Q(s_t,a_t) = R_{t+1} + \gamma max_{a+1}Q(s_{t+1},a_{t+1})$$</p>
위의 식에서 우리는 Reward Matrix(<span>$$R_{t}$$</span>)는 구하였고, Discount Factor(<span>$$\gamma$$</span>)는 0 ~ 1의 값으로 지정하는 값 이다.  
따라서 최종적인 식에 대하여 Q Matrix(<span>$$Q_{t}$$</span>)를 구할 수 있다.  

<br>
<span>$$\gamma$$</span>를 0.8로서 지정하고 Q Matrix를 0행렬로 초기화하면 다음과 같은 상태로 나타낼 수 있다.  
<img alt="" class="se_mediaImage __se_img_el _lazy-loading-target-image" data-attachment-id="IWMmUzPldvEHSH2whhdpre6-FWVM" src="https://mblogthumb-phinf.pstatic.net/20160822_112/infoefficien_1471857329683M0tpC_JPEG/%C6%F7%B8%CB%BA%AF%C8%AF_7.jpg?type=w800" data-lazy-src="" data-width="304" data-height="202" id="img_9" data-top="4908.5498046875"/><br>
<img alt="" class="se_mediaImage __se_img_el _lazy-loading-target-image" data-attachment-id="IdJAojoxuUK48qiY4IuRouNsTmgw" src="https://mblogthumb-phinf.pstatic.net/20160822_223/infoefficien_1471857329678PXTwB_JPEG/%C6%F7%B8%CB%BA%AF%C8%AF_8.jpg?type=w800" data-lazy-src="" data-width="227" data-height="177" id="img_10" data-top="4908.5498046875"/><br>
사진 출처: <a href="https://m.blog.naver.com/PostView.nhn?blogId=infoefficien&logNo=220769665748&proxyReferer=https%3A%2F%2Fwww.google.com%2F">gaussian37</a><br><br>

**위의 그림과 같은 상황에서 Q-Matix를 Update시키는 방법에 대하여 살펴보자.**  
먼저 다음과 같은 상황을 가정하자.
- State: 1
- Action: 1 -> 5

위의 가정을 생각하고 식을 대입하면 다음과 같다.  
<p>$$Q(1,5) = R(1,5) + 0.8 max(Q(5,1), Q(5,4), Q(5,5)) = 100 + 0.8*0 = 100$$</p>
위의 결과를 Q-Matix에 대해여 Update시키면 다음과 같다.  
<img src="https://mblogthumb-phinf.pstatic.net/20160822_143/infoefficien_1471857353075oi4wp_JPEG/%C6%F7%B8%CB%BA%AF%C8%AF_9.jpg?type=w800"/><br>
사진 출처: <a href="https://m.blog.naver.com/PostView.nhn?blogId=infoefficien&logNo=220769665748&proxyReferer=https%3A%2F%2Fwww.google.com%2F">gaussian37</a><br><br>
위와 같은 방법으로 Update를 시키면서 최종적인 Q-Matrix를 Edge와 Node롯 표현하면 다음과 같은 결과를 얻을 수 있다.  
<img src="https://mblogthumb-phinf.pstatic.net/20160822_60/infoefficien_1471857370324XJCOe_JPEG/%C6%F7%B8%CB%BA%AF%C8%AF_13.jpg?type=w800"/><br>
사진 출처: <a href="https://m.blog.naver.com/PostView.nhn?blogId=infoefficien&logNo=220769665748&proxyReferer=https%3A%2F%2Fwww.google.com%2F">gaussian37</a><br><br>
최종적인 결과를 살펴보게 되면 모든 State에 대한 Action의 값(Reward)을 비교해보면 동일한 State에서는 Goal에 가까이 가는 Action의 값이 더 높은 것을 확인할 수 있다.  

#### Q-Network
위의 방법의 경우 <a href="https://wjddyd66.github.io/dl/NeuralNetwork-(3)-Optimazation/">
NeuralNetwork (3) Optimazation </a>와 같이 행렬로서 표현하게 되면 연산량이 많아지게 되고 부하가 걸릴 확률이 매우 높으므로 Normal Equation이 아닌 GSD를 사용하였듯이 **강화 학습도 Q-Table이 아닌 대부분 Q-Network를 통하여 학습을 진행하게 된다.**  
Q-Network의 식은 아래와 같이 나타낼 수 있다.  
<p>$$MSE = \frac{1}{2n} \sum+{i=1}^{n}(R_{t+1} + \gamma max_{a+1}Q(S_{t+1},a_{t+1}) - Q(s_t,a_t))^{2}$$</p>
기본적인 ANN Network구조에서 Loss Function으로서 MSE를 사용한 것을 확인할 수 있다.  
조심해야하는 상황은 만약 위에식 대로 항상 최적의 방향으로만 움직이는 상황에 대해서 생각해보자.  
**그렇게 되면 항상 같은 방향으로 움직이기 때문에 데이터의 수집의 다향성을 잃게되고 또한 Local Minima에 빠질 수 있다.**  
**따라서 Q-Network에서는 보통 입실론(<span>$$\epsilon$$</span>)-Greedy 방법을 사용한다.**  
<span>$$\epsilon$$</span>-Greedy방법이란 <span>$$\epsilon$$</span>확률로 Agent가 최적의 행동이 아닌 랜덤한 행동을 하고 1-<span>$$\epsilon$$</span>의 확률로 최적의 행동을 하게 된다.  
Epoch에 따라서 <span>$$\epsilon$$</span>의 값은 조금씩 낮추는 방향으로 진행하게 된다.  

**Q-Network의 경우에도 Q-Table처럼 자세히 알아보기 위하여 다음 <a href="https://www.youtube.com/watch?v=w9GwqPx7LW8&feature=youtu.be">김성훈 교수님 강좌</a>를 보면서 정리한 내용을 살펴보자.**  
기본적으로 Q-Table을 Q-Network로서 구성한다고 가정하면 다음과 같은 Network의 구조로서 나타낼 수 있다.  
<img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/169.png" height="100%" width="100%" /><br>
사진 출처: <a href="https://www.youtube.com/watch?v=w9GwqPx7LW8&feature=youtu.be">Sung Kim Youtube</a><br>
위의 사진을 살펴보게 되면 Action과 State를 Input으로 넣고 Network를 지나서 Output으로서 Reward가 출력되는 구조이다.  
위의 Network를 간단히 나타내기 위하여 아래와 같은 구조로서 바꾸어 보자.  
<img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/170.png" height="100%" width="100%" /><br>
사진 출처: <a href="https://www.youtube.com/watch?v=w9GwqPx7LW8&feature=youtu.be">Sung Kim Youtube</a><br>
위의 사진을 살펴보게 되면 Input으로서는 State에 대한 Input만 들어가게 되고 Output은 모든 Action에 대한 Value가 출력되게 된다.  
이제 Input에 대하여 정의하였으므로 CostFunction을 MSE라고 한다면 Label을 정의해야지 결과적으로 Loss를 구하고 Backpropagation을 통하여 weight를 Update할 수 있다.  
이에 관한 사진은 아래를 살펴보자.  
<img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/171.png" height="100%" width="100%" /><br>
사진 출처: <a href="https://www.youtube.com/watch?v=w9GwqPx7LW8&feature=youtu.be">Sung Kim Youtube</a><br>
**위의 사진으로서 어느정도 개념을 잡을 수 있다. 결과적으로 Label을 구하는 방식은 Q-Table과 같으나 Update와 연산에 필요한 Hyperparameter를 줄여서 Network구조로서 나타내는 것을 Q-Network라고 할 수 있다.**  
위의 결과는 다음과 같은 식으로서 나타낼 수 있다.  
<p>$$\hat{Q}(s,a|\theta)(= Ws \text{, }\theta = \text{ Weight}) ~ Q^{*}(s,a)(Label)$$</p>
위의 식으로서 표현한 것을 LossFunction MSE에 대입하면 최종적으로 다음과 같은 식을 얻을 수 있다.  
<p>$$min_{\theta}\sum_{t=0}^{T}[\hat{Q}(s,a|\theta) - (r_t + \gamma max_{a^{'}}\hat{Q(s_{t+1},a^{'}|\theta)})]$$</p>
이러한 Network의 구조는 다음과 같은 2가지의 문제로 인하여 **diverges(발산)**가 일어나게 된다.
- Correlations between samples
- Non-sationary targets

**Correlations between samples**  
어떠한 Action에 대한 State와 Rewards가 비슷하다는 문제가 발생하게 된다.  
이러한 Data들은 Correlation이 나타나게 되고 이러한 결과는 **기본적인 Network의 Activation Function을 적용시키는 이유인 Non Linearity가 제거되고 Linear한 상태가 된다.**  

**Non-sationary targets**  
위의 최종적인 식을 살펴보게 되면 MSE에 들어가게 되는 2개의 Data는 결국 1 Step밖에 차이나지 않는 값이며 이는 서로 매우 가까움을 의미하게 된다.  
**이러한 매우 가까운 값을 사용하게 되면, Targe의 값이 지속적으로 Shift될 수 있다.**  
즉, 다음 값을 예측하여 현재 Weight를 변경하고 바로 다음 Epoch로 넘어가게 되면 이러한 Target이 다음 Epoch에 의해 변화되는 과정이 무한정 반복하게 되면서 점점 Target은 원래의 위치에서 Shift되는 현상이 발생할 수 있다는 것 이다.  
즉, 기본적인 Network로서는 공유되는 Weight를 공유하고 있어서 Weight를 Update시킴으로 인하여 다음 Epoch에 영향을 미치는 문제이다.  
**이러한 Diverges한 문제를 DQN(Deep, Replay, Separated Network)로서 해결하였다.**  

<br><br>

### DQN
DQN을 들어가기 먼저 위에서도 언급하였듯이 **DQN은 Diverges한 문제를 해결하기 위한 Network**이다.  
이러한 Network를 어떻게 구현하였는지 살펴보자.  
기본적인 DQN이 제시한 해결방법은 3가지이다.  
1. Go Deep: Network를 깊게 쌓는다.
2. Capture and replay: Correlations between samples 해결
3. Separate Networks: Non-stationart targets 해결

각각의 해결 방안이 어떻게 문제를 해결하였는지 살펴보자.  
<br>
**Go Deep**  
<img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/172.png" height="100%" width="100%" /><br>
사진 출처: <a href="https://www.youtube.com/watch?v=w9GwqPx7LW8&feature=youtu.be">Sung Kim Youtube</a><br>
가장 Network의 효율을 올릴수 있는 간단한 방법이다.  
항상 Network를 Deep하게 쌓게되면 비선형은 증가하게 될 것이고 Accuracy는 증가할 수 밖에 없다.  
단. Overfiting과 Vanishing같은 문제는 발생할 수 있다.  
하나 주목해야 할 점은 DQN의 Network Layer는 ANN을 사용한 것이 아니라 CNN을 사용하였다는 것 이다.(하지만 예제는 간단한 문제이므로 CNN으로 Layer를 구성하지 않고 ANN으로서 Network의 Layer를 구성하였다.)  
<br>

**Capture and replay**  
Correlations between samples를 해결하기 위한 방법이다.  
일단 아래 사진을 살펴보자.  
<img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/173.png" height="100%" width="100%" /><br>
사진 출처: <a href="https://www.youtube.com/watch?v=w9GwqPx7LW8&feature=youtu.be">Sung Kim Youtube</a><br>
위의 사진을 살펴보게 되면 기본적인 구조와 다르다.  
**위의 사진에서 중요한 것은 하나의 결과에 대하여 Train을 실시하는 것이 아니라 Buffer에 결과를 모아서 저장하게 되고, Buffer에서 Random하게 몇 개의 결과를 뽑아서 Train을 실시하게 함으로써 Correlation and replay문제를 해결하였다.**  
이러한 결과는 Minibatch로서 모아와서 Train을 진행하게 된다.  
Data의 분포에서 Random한 몇개를 가져와도 전체 분포와 비슷하게 이루워질 것 이라는 생각이다.  
개인적으로는 DQN뿐만 아니라 다른 많은 Network에서도 적용시킬 수 있는 좋은 Algorithm방법인 것 같다.  
<br>

**Separate Networks**  
Separate Networks로서 말 그대로 Network를 여러개 적용하여 Train 실시한다는 것 이다.  
원래 식은 아래와 같다.  
<p>$$min_{\theta}\sum_{t=0}^{T}[\hat{Q}(s,a|\theta) - (r_t + \gamma max_{a^{'}}\hat{Q(s_{t+1},a^{'}|\theta)})]$$</p>
위의 식에서의 문제는 weight(<span>$$\theta$$</span>)를 공유한다는 것이다.  
아래 식은 DQN에서 Separate Networks를 해결하기 위한 식 이다.  
<p>$$min_{\theta}\sum_{t=0}^{T}[\hat{Q}(s,a|\theta) - (r_t + \gamma max_{a^{'}}\hat{Q(s_{t+1},a^{'}|\bar{\theta})})]$$</p>
위의 식을 살펴보게 되면 각각의 Network는 <span>$$\theta$$</span>, <span>$$\bar{\theta}$$</span>로서 서로 다른 Weight를 가지게 됨으로써 Non-sationary targets문제를 해결할 수 있다.  
<img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/174.png" height="100%" width="100%" /><br>
사진 출처: <a href="https://www.youtube.com/watch?v=w9GwqPx7LW8&feature=youtu.be">Sung Kim Youtube</a><br>
위의 그림과 식을 살펴보게 되면 다음과 같은 과정을 거치게 된다.  
1) Network를 Prediction과 Target으로서 2개의 Network구조로서 바꾼다.  
각각의 Network의 Hyperparameter는 다음과 같다.  
- Prediction: <span>$$\theta$$</span>
- Target: <span>$$\bar{\theta}$$</span>

2) Prediction: <span>$$\theta$$</span>를 Update 시킨다.  
3) 일정 Training 후 <span>$$\bar{\theta}$$</span>는 <span>$$\theta$$</span>를 복사한다.  

<br><br>

### train_catch_game.py


<br>
**결과 확인**  
```code
반복(Epoch): 0, 에러(err): 0.0095, 승리횟수(Win count): 1, 승리비율(Win ratio): 100.0000
반복(Epoch): 1, 에러(err): 0.0661, 승리횟수(Win count): 2, 승리비율(Win ratio): 100.0000
반복(Epoch): 2, 에러(err): 0.1191, 승리횟수(Win count): 2, 승리비율(Win ratio): 66.6667
반복(Epoch): 3, 에러(err): 0.2151, 승리횟수(Win count): 2, 승리비율(Win ratio): 50.0000

...

반복(Epoch): 1999, 에러(err): 0.0000, 승리횟수(Win count): 1823, 승리비율(Win ratio): 91.1500
반복(Epoch): 2000, 에러(err): 0.0000, 승리횟수(Win count): 1824, 승리비율(Win ratio): 91.1544
트레이닝 완료
/home/jyhwang/deep-learning-tensorflow-book-code/Ch12-DQN/model.ckpt 경로에 파라미터가 저장되었습니다
```
<br>
<br><br>

### play_catch_game.ipynb

**결과 확인**  
<video src="dqn.mp4" autoplay controls>    
브라우저가 비디오를 지원하지 않습니다.
</video>

<br><br>

<hr>
참조:<a href="https://github.com/wjddyd66/Tensorflow/tree/master/FCN">원본코드</a><br>
참조: <a href="https://bskyvision.com/491">bskyvision.com</a><br>
참조: <a href="https://github.com/shekkizh/FCN.tensorflow">shekkizh GitHub</a><br>
참조: <a href="https://modulabs-biomedical.github.io/">modulabs-biomedical 블로그</a><br>
참조: <a href="http://ataspinar.com/2017/12/04/using-convolutional-neural-networks-to-detect-features-in-sattelite-images/">ataspinar.com</a><br>
참조:텐서플로로 배우는 딥러닝<br>
문제가 있거나 궁금한 점이 있으면 wjddyd66@naver.com으로  Mail을 남겨주세요.
