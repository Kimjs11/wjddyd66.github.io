---
layout: post
title:  "Tensorflow-DQN"
date:   2019-12-01 09:00:00 +0700
categories: [Tensorflow]
---
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
### 강화학습
강화학습이란, **주어진 어떤 상황(State)에서 보상(Reward)을 최대화 할 수 있는 행동(Action)에 대해 학습하는 것**이다.  
위의 과정을 위해서 학습의 **주체(Agent)가 상황에 가장 적합한 행동을 찾기까지는 수많은 시행착오가 필요하다.**  
위의 과정을 사진으로 나타내면 아래와 같다.  
<img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/165.png" height="100%" width="100%" /><br>
사진 출처: <a href="https://brunch.co.kr/@kakao-it/73">Kakao 블로그</a><br>
위의 각각의 요소를 자세히 나누면 다음과 같다.  
- S: 상태(State)들의 집합
- A: 행동(Action)들의 집합
- R: 보상(Reward)들의 집합
- R(<span>$$s,s^{'}$$</span>): 행동 a에 의해서 상태 s에서 다음 상태 <span>$$s^{'}$$</span>로 넘어갈때 얻는 보상(Rewoard)
- P(<span>$$s,s^{'}$$</span>): 상태 s에서 다음 상태 <span>$$s^{'}$$</span>로 넘어갈 전이 확률(Transition Probability)
- <span>$$\gamma$$</span>: 형재 보상과 미래 보상의 중요도를 조정하는 Discount Factor


위의 예시로서 좋은 벽돌깨기 게임을 생각해보자.  
<img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/166.png" height="100%" width="100%" /><br>
위의 사진과 같은 Game에서 **Ageng(Bar)**가 할 수 있는 **Action**은 3가지이다.  
1) 움직이지 않기  
2) 좌로 움직이기  
3) 우로 움직이기  

다음과 같은 3가지의 Acion에 대해서 만약 **공이 많은 벽돌을 깨뜨리면 많은 보상(Reward)를 얻을 수 있고, 만약 죽게되면 적은 보상(Reward)를 얻을 수 있다.**  
따라서 각각의 Image에 따라서 Agent는 어떤 Action을 취해야지 더 많은 보상(Reward)를 얻을 수 있는지에 대해 계속해서 Training을 실시한다.  
이러한 Agent가 현재 상태에서 어떻게 행동할 것인지를 결정하는 전략을 <span>$$\pi$$</span>로 표현하고 최적의 정책은 <span>$$\pi^{*}$$</span>를 찾는 것을 학습의 목표로서 표현한다.  
이러한 <span>$$\pi^{*}$$</span>을 찾기 위해서는 **상태 가치 함수와 행동 가치 함수**라는 개념을 사용한다.  

<br><br>

### 상태 가치 함수, 행동 가치 함수

<br><br>

### Q-Learning

<br><br>

### DQN

<br><br>

### train_catch_game.py


<br>
**결과 확인**  
```code
반복(Epoch): 0, 에러(err): 0.0095, 승리횟수(Win count): 1, 승리비율(Win ratio): 100.0000
반복(Epoch): 1, 에러(err): 0.0661, 승리횟수(Win count): 2, 승리비율(Win ratio): 100.0000
반복(Epoch): 2, 에러(err): 0.1191, 승리횟수(Win count): 2, 승리비율(Win ratio): 66.6667
반복(Epoch): 3, 에러(err): 0.2151, 승리횟수(Win count): 2, 승리비율(Win ratio): 50.0000

...

반복(Epoch): 1999, 에러(err): 0.0000, 승리횟수(Win count): 1823, 승리비율(Win ratio): 91.1500
반복(Epoch): 2000, 에러(err): 0.0000, 승리횟수(Win count): 1824, 승리비율(Win ratio): 91.1544
트레이닝 완료
/home/jyhwang/deep-learning-tensorflow-book-code/Ch12-DQN/model.ckpt 경로에 파라미터가 저장되었습니다
```
<br>
<br><br>

### play_catch_game.ipynb

**결과 확인**  
<video src="../static/img/AI/dqn.mp4" autoplay controls>    
브라우저가 비디오를 지원하지 않습니다.
</video>

<br><br>

<hr>
참조:<a href="https://github.com/wjddyd66/Tensorflow/tree/master/FCN">원본코드</a><br>
참조: <a href="https://bskyvision.com/491">bskyvision.com</a><br>
참조: <a href="https://github.com/shekkizh/FCN.tensorflow">shekkizh GitHub</a><br>
참조: <a href="https://modulabs-biomedical.github.io/">modulabs-biomedical 블로그</a><br>
참조: <a href="http://ataspinar.com/2017/12/04/using-convolutional-neural-networks-to-detect-features-in-sattelite-images/">ataspinar.com</a><br>
참조:텐서플로로 배우는 딥러닝<br>
문제가 있거나 궁금한 점이 있으면 wjddyd66@naver.com으로  Mail을 남겨주세요.
