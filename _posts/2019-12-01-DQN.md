---
layout: post
title:  "Tensorflow-DQN"
date:   2019-12-01 09:00:00 +0700
categories: [Tensorflow]
---
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
### 강화학습
강화학습이란, **주어진 어떤 상황(State)에서 보상(Reward)을 최대화 할 수 있는 행동(Action)에 대해 학습하는 것**이다.  
위의 과정을 위해서 학습의 **주체(Agent)가 상황에 가장 적합한 행동을 찾기까지는 수많은 시행착오가 필요하다.**  
위의 과정을 사진으로 나타내면 아래와 같다.  
<img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/165.png" height="100%" width="100%" /><br>
사진 출처: <a href="https://brunch.co.kr/@kakao-it/73">Kakao 블로그</a><br>
위의 각각의 요소를 자세히 나누면 다음과 같다.  
- S: 상태(State)들의 집합
- A: 행동(Action)들의 집합
- R: 보상(Reward)들의 집합
- R(<span>$$s,s^{'}$$</span>): 행동 a에 의해서 상태 s에서 다음 상태 <span>$$s^{'}$$</span>로 넘어갈때 얻는 보상(Rewoard)
- P(<span>$$s,s^{'}$$</span>): 상태 s에서 다음 상태 <span>$$s^{'}$$</span>로 넘어갈 전이 확률(Transition Probability)
- <span>$$\gamma$$</span>: 현재 보상과 미래 보상의 중요도를 조정하는 Discount Factor


위의 예시로서 좋은 벽돌깨기 게임을 생각해보자.  
<img src="https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/166.png" height="100%" width="100%" /><br>
위의 사진과 같은 Game에서 **Ageng(Bar)**가 할 수 있는 **Action**은 3가지이다.  
1) 움직이지 않기  
2) 좌로 움직이기  
3) 우로 움직이기  

다음과 같은 3가지의 Acion에 대해서 만약 **공이 많은 벽돌을 깨뜨리면 많은 보상(Reward)를 얻을 수 있고, 만약 죽게되면 적은 보상(Reward)를 얻을 수 있다.**  
따라서 각각의 Image에 따라서 Agent는 어떤 Action을 취해야지 더 많은 보상(Reward)를 얻을 수 있는지에 대해 계속해서 Training을 실시한다.  
이러한 Agent가 현재 상태에서 어떻게 행동할 것인지를 결정하는 전략을 <span>$$\pi$$</span>로 표현하고 최적의 정책은 <span>$$\pi^{*}$$</span>를 찾는 것을 학습의 목표로서 표현한다.  
이러한 <span>$$\pi^{*}$$</span>을 찾기 위해서는 **상태 가치 함수와 행동 가치 함수**라는 개념을 사용한다.  

<br><br>

### 상태 가치 함수, 행동 가치 함수

**상태 가치 함수**  
**상태 가치 함수란 현재 상태의 좋음과 나쁨을 표현한다.**  
상태 가치 함수는 아래 수식으로 표현된다.  
<p>$$V_{\pi}(s) = E_{\pi}[R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+ ... |S_{t}=s]$$</p>
위의 수식을 살펴보게 되면 위에서 어떠게 행동할 것인지 결정하는 전략 <span>$$\pi$$</span>에 대해서 현재 상태 (<span>$$S _ {t}$$</span>)에 대한 보상(<span>$$R_{t}, R_{t+1}, ...$$</span>)으로서 표현한다.  
위에서 <span>$$\gamma$$</span>는 현재 보상과 미래 보상의 중요도를 조정하는 Disount Factor로서 0 ~ 1사이의 값으로서 표현된다.  
즉, 현재 시점 (t)에서의 상태 가치 함수는 <span>$$\gamma \neq 0$$</span>이면 미래의 시간에 대한 보상까지 생각하여 계산한 값이지만, 통상적으로 Reward가 매우 크지 않으면 가까운 시간에 영향을 많이 받는 값으로서 표현된다는 거 이다.  
만약 미로 찾기에서 Goal을 찾아가는 문제로 나타내게 되면 아래 그림과 같이 표현할 수 있다.  
<img src="https://postfiles.pstatic.net/MjAxODEyMjRfMjA0/MDAxNTQ1NjMwMjAzOTgx.qudSFCatdMk3sNvnFsVckednkL6wEVOdAt3q4KmQqwYg.k3q3foMlNLZqohPQtVEYpv41mw2YAkJ1R3AuonlY08kg.JPEG.horajjan/SE-2277fa04-3b8c-4d9b-99e9-88a96c5cb170.jpg?type=w773" /><br>
사진 출처: <a href="https://blog.naver.com/PostView.nhn?blogId=horajjan&logNo=221426201763&parentCategoryNo=&categoryNo=122&viewDate=&isShowPopularPosts=true&from=search">일체유심조 Blog</a>  
각각이 그림은 <span>$$\gamma$$</span>의 상태에 따라 달라진다.  
또한 이해가 바로가는 책에서의 예제는 다음과 같이 나와있다.  
> 스타크래프트 게임을 플레이하는 상황을 가정해보면 초반에 일꾼을 이용해서 미니맵을 정찰하는 것은 당장에는 보상이 없지만 미래에 어떤 전력을 펼칠지 결저알 수 있도록 도와주는 중요한 정보를 얻을 수 있는 행동입니다.

<br>
**행동 가치 함수**  
**행동 가치 함수는 현재 행동의 좋음과 나쁨을 표현한다.**  
실질적인 강화학습에서는 Agent의 행동에 따른 보상을 제공한다.  
따라서 앞으로의 방식은 행동 가치 함수를 통하여 Training하고 결과를 확인하는 과정이 될 것이다.  
행동 가치 함수를 수식으로 살펴보면 다음과 같다.  
<p>$$Q_{\pi}(s,a) = E_{\pi}[R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+ ... |S_{t}=s,A_{t}=a]$$</p>
위의 수식을 살펴보게 되면 상태가치 함수와 식이 비슷하나 다른것은 **State 뿐만 아니라 Action에 대해서도 고려를 한다는 것** 이다.  
만약 미로 찾기에서 Goal을 찾아가는 문제로 나타내게 되면 아래 그림과 같이 표현할 수 있다.  
<img src="https://postfiles.pstatic.net/MjAxODEyMjRfMjYy/MDAxNTQ1NjMwOTU1NjYw.A6QqBXOZ6LQo98-jGPvvjQ9I8K3sOL0m3fUNFbpOmiIg.WppfhFtgO14ZF9GD2hmIiIb_W0OiMWR1v3cEn8tZRKIg.JPEG.horajjan/SE-ddc16ffb-f98a-414b-9ec0-f1c353c1ac35.jpg?type=w773" /><br>
사진 출처: <a href="https://blog.naver.com/PostView.nhn?blogId=horajjan&logNo=221426201763&parentCategoryNo=&categoryNo=122&viewDate=&isShowPopularPosts=true&from=search">일체유심조 Blog</a>  
참고로 상태 가치 함수를 이용해서 알아내는 방법을 Planning이라고 하고 행동 가치 함수를 이용해서 알아내는 방법을 강화 학습 이라고 한다.  
<br><br>

### Q-Learning
Q-Learning은 적절한 행동 가치 함수값을 알아내기 위한 알고리즘이다.  
수학적인 수식으로서 살펴보게 되면 식은 아래와 같다.  
<p>$$Q(s_t,a_t) = R_{t+1} + \gamma max_{a+1}Q(s_{t+1},a_{t+1})$$</p>
솔직히 처음 식을 보고 Update를 어떻게 시킬 것인지 혹은 각각의 행렬은 무슨 의미를 가지는지 매우 어렵다.  
따라서 좋은 예제인 <a href="https://m.blog.naver.com/PostView.nhn?blogId=infoefficien&logNo=220769665748&proxyReferer=https%3A%2F%2Fwww.google.com%2F">gaussian37</a>의 Blog의 예제를 살펴보자.  

#### Q-Table
먼저 아래와 같은 사진의 문제를 정의하자.  
<img src="https://mblogthumb-phinf.pstatic.net/20160822_279/infoefficien_1471857210056fyKks_JPEG/%C6%F7%B8%CB%BA%AF%C8%AF_1.jpg?type=w800"/><br>
사진 출처: <a href="https://m.blog.naver.com/PostView.nhn?blogId=infoefficien&logNo=220769665748&proxyReferer=https%3A%2F%2Fwww.google.com%2F">gaussian37</a><br>
위와 같이 5개의 방이 있다고 가정한다. 5번방이 Goal이라고 생각하면 위의 사진은 아래와 같이 node와 edge로서 표현할 수 있다.  
<img src="https://mblogthumb-phinf.pstatic.net/20160822_293/infoefficien_1471857224998pEtY9_JPEG/%C6%F7%B8%CB%BA%AF%C8%AF_2.jpg?type=w800"/><br>
사진 출처: <a href="https://m.blog.naver.com/PostView.nhn?blogId=infoefficien&logNo=220769665748&proxyReferer=https%3A%2F%2Fwww.google.com%2F">gaussian37</a><br><br>
Goal로 가는 Action의 값을 100이라 두고 다른 Action은 모두 0이라고 가정하면 아래 그림과 같이 나타낼 수 있다.  
<img src="https://mblogthumb-phinf.pstatic.net/20160822_76/infoefficien_1471857235025yoYMX_JPEG/%C6%F7%B8%CB%BA%AF%C8%AF_3.jpg?type=w800"/><br>
사진 출처: <a href="https://m.blog.naver.com/PostView.nhn?blogId=infoefficien&logNo=220769665748&proxyReferer=https%3A%2F%2Fwww.google.com%2F">gaussian37</a><br><br>
위의 Node와 Edge를 행렬을 다음의 조건에 적용시켜 행렬로 나타내보자.  
- 상태(State)는 현재 방의 위치이다.
- 행동(Action)은 다음 방으로 이동할 수 있는 경로 이다.
- 이동할 수 있으면 0, Goalㄹ 이동할 수 있으면 100, 이동할 수 없으면 -1로서 Action의 값을 할당한다.

<br>
<img src="https://mblogthumb-phinf.pstatic.net/20160822_94/infoefficien_1471857319337AH04p_JPEG/%C6%F7%B8%CB%BA%AF%C8%AF_6.jpg?type=w800"/><br>
사진 출처: <a href="https://m.blog.naver.com/PostView.nhn?blogId=infoefficien&logNo=220769665748&proxyReferer=https%3A%2F%2Fwww.google.com%2F">gaussian37</a><br><br>
위의 Matrix는 결과적으로 **Agent가 State에서 Action을 취함으로써 얻을 수 있는 Reward Matrix**가 된다.  
여기서 잠깐 Q-Learning 식을 다시한번 살펴보자.  
<p>$$Q(s_t,a_t) = R_{t+1} + \gamma max_{a+1}Q(s_{t+1},a_{t+1})$$</p>
위의 식에서 우리는 Reward Matrix(<span>$$R_{t}$$</span>)는 구하였고, Discount Factor(<span>$$\gamma$$</span>)는 0 ~ 1의 값으로 지정하는 값 이다.  
따라서 최종적인 식에 대하여 Q Matrix(<span>$$Q_{t}$$</span>)를 구할 수 있다.  

<br>
<span>$$\gamma$$</span>를 0.8로서 지정하고 Q Matrix를 0행렬로 초기화하면 다음과 같은 상태로 나타낼 수 있다.  
<img alt="" class="se_mediaImage __se_img_el _lazy-loading-target-image" data-attachment-id="IWMmUzPldvEHSH2whhdpre6-FWVM" src="https://mblogthumb-phinf.pstatic.net/20160822_112/infoefficien_1471857329683M0tpC_JPEG/%C6%F7%B8%CB%BA%AF%C8%AF_7.jpg?type=w800" data-lazy-src="" data-width="304" data-height="202" id="img_9" data-top="4908.5498046875"/><br>
<img alt="" class="se_mediaImage __se_img_el _lazy-loading-target-image" data-attachment-id="IdJAojoxuUK48qiY4IuRouNsTmgw" src="https://mblogthumb-phinf.pstatic.net/20160822_223/infoefficien_1471857329678PXTwB_JPEG/%C6%F7%B8%CB%BA%AF%C8%AF_8.jpg?type=w800" data-lazy-src="" data-width="227" data-height="177" id="img_10" data-top="4908.5498046875"/><br>
사진 출처: <a href="https://m.blog.naver.com/PostView.nhn?blogId=infoefficien&logNo=220769665748&proxyReferer=https%3A%2F%2Fwww.google.com%2F">gaussian37</a><br><br>

**위의 그림과 같은 상황에서 Q-Matix를 Update시키는 방법에 대하여 살펴보자.**  
먼저 다음과 같은 상황을 가정하자.
- State: 1
- Action: 1 -> 5

위의 가정을 생각하고 식을 대입하면 다음과 같다.  
<p>$$Q(1,5) = R(1,5) + 0.8 max(Q(5,1), Q(5,4), Q(5,5)) = 100 + 0.8*0 = 100$$</p>
위의 결과를 Q-Matix에 대해아ㅕ Update시키면 다음과 같다.  
<img src="https://mblogthumb-phinf.pstatic.net/20160822_143/infoefficien_1471857353075oi4wp_JPEG/%C6%F7%B8%CB%BA%AF%C8%AF_9.jpg?type=w800"/><br>
사진 출처: <a href="https://m.blog.naver.com/PostView.nhn?blogId=infoefficien&logNo=220769665748&proxyReferer=https%3A%2F%2Fwww.google.com%2F">gaussian37</a><br><br>
위와 같은 방법으로 Update를 시키면서 최종적인 Q-Matrix를 Edge와 Node롯 표현하면 다음과 같은 결과를 얻을 수 있다.  
<img src="https://mblogthumb-phinf.pstatic.net/20160822_60/infoefficien_1471857370324XJCOe_JPEG/%C6%F7%B8%CB%BA%AF%C8%AF_13.jpg?type=w800"/><br>
사진 출처: <a href="https://m.blog.naver.com/PostView.nhn?blogId=infoefficien&logNo=220769665748&proxyReferer=https%3A%2F%2Fwww.google.com%2F">gaussian37</a><br><br>
최종적인 결과를 살펴보게 되면 모든 State에 대한 Action의 값(Reward)을 비교해보면 동일한 State에서는 Goal에 가까이 가는 Action의 값이 더 높은 것을 확인할 수 있다.  

#### Q-Network
위의 방법의 경우 <a href="https://wjddyd66.github.io/dl/NeuralNetwork-(3)-Optimazation/">
NeuralNetwork (3) Optimazation </a>와 같이 행렬로서 표현하게 되면 연산량이 많아지게 되고 부하가 걸릴 확률이 매우 높으므로 Normal Equation이 아닌 GSD를 사용하였듯이 **강화 학습도 Q-Table이 아닌 대부분 Q-Network를 통하여 학습을 진행하게 된다.**  
Q-Network의 식은 아래와 같이 나타낼 수 있다.  
<p>$$MSE = \frac{1}{2n} \sum+{i=1}^{n}(R_{t+1} + \gamma max_{a+1}Q(S_{t+1},a_{t+1}) - Q(s_t,a_t))^{2}$$</p>
기본적인 ANN Network구조에서 Loss Function으로서 MSE를 사용한 것을 확인할 수 있다.  


<br><br>

### DQN

<br><br>

### train_catch_game.py


<br>
**결과 확인**  
```code
반복(Epoch): 0, 에러(err): 0.0095, 승리횟수(Win count): 1, 승리비율(Win ratio): 100.0000
반복(Epoch): 1, 에러(err): 0.0661, 승리횟수(Win count): 2, 승리비율(Win ratio): 100.0000
반복(Epoch): 2, 에러(err): 0.1191, 승리횟수(Win count): 2, 승리비율(Win ratio): 66.6667
반복(Epoch): 3, 에러(err): 0.2151, 승리횟수(Win count): 2, 승리비율(Win ratio): 50.0000

...

반복(Epoch): 1999, 에러(err): 0.0000, 승리횟수(Win count): 1823, 승리비율(Win ratio): 91.1500
반복(Epoch): 2000, 에러(err): 0.0000, 승리횟수(Win count): 1824, 승리비율(Win ratio): 91.1544
트레이닝 완료
/home/jyhwang/deep-learning-tensorflow-book-code/Ch12-DQN/model.ckpt 경로에 파라미터가 저장되었습니다
```
<br>
<br><br>

### play_catch_game.ipynb

**결과 확인**  
<video src="dqn.mp4" autoplay controls>    
브라우저가 비디오를 지원하지 않습니다.
</video>

<br><br>

<hr>
참조:<a href="https://github.com/wjddyd66/Tensorflow/tree/master/FCN">원본코드</a><br>
참조: <a href="https://bskyvision.com/491">bskyvision.com</a><br>
참조: <a href="https://github.com/shekkizh/FCN.tensorflow">shekkizh GitHub</a><br>
참조: <a href="https://modulabs-biomedical.github.io/">modulabs-biomedical 블로그</a><br>
참조: <a href="http://ataspinar.com/2017/12/04/using-convolutional-neural-networks-to-detect-features-in-sattelite-images/">ataspinar.com</a><br>
참조:텐서플로로 배우는 딥러닝<br>
문제가 있거나 궁금한 점이 있으면 wjddyd66@naver.com으로  Mail을 남겨주세요.
