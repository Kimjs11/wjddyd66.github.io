---
layout: post
title:  "Theory5. SVM"
date:   2020-04-25 09:50:20 +0700
categories: [Machine Learning]
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## 5. SVM
$$\newcommand{\argmin}{\mathop{\mathrm{argmin}}\limits}$$
$$\newcommand{\argmax}{\mathop{\mathrm{argmax}}\limits}$$
Machine Learning의 기초적인 이론부분을 다시 제대로 잡고 싶어서 <a href="https://kaist.edwith.org/machinelearning1_17/joinLectures/9738">문일철 교수님의 인공지능 및 기계학습 개론</a>을 정리한 Post입니다.

- 5.1 Decision Boundary with Margin 
- 5.2 Maximizing the Margin
- 5.3 Soft Margin with SVM
- 5.4 Comparison to Logistic Regression
- 5.5 Rethinking of SVM
- 5.6 Primal and Dual with KKT Condition
- 5.7 Kernel
- 5.8 SVM with Kernel

### 5.1 Decision Boundary with Margin
SVM이란 Classification의 일종이다.  

**SVM이란 이러한 상황에서 수많은 Decision Boundary에서 각각의 가장 가까운 Point 사이의 거리(Margin)을 최대화 하는 것 이다.**

<img src="https://miro.medium.com/max/609/0*lyr5-f7HRu34OLvd.png"><br>
사진 출처: <a href="https://medium.com/@unfinishedgod/r-%EC%8B%A0%EC%9A%A9%EB%B6%84%EC%84%9D-%EC%98%88%EC%B8%A1%EC%A0%81%EB%B6%84%EC%84%9D-%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%AA%A8%EB%8D%B8%EB%A7%81-13e17af3e7ed">medium.com</a>

수식으로 나타내면 다음과 같다.  

<p>$$f(x) = wx+b$$</p>
Point가 x가로 하는 경우 Decision Boundary(Hyperplane)는 <span>$$f(x) = wx+b=0$$</span>으로 잡을 수 있다.  


만약, Support Vector의 값이 <span>$$f(x) = wx+b=a$$</span>라면, Input Point에 따라서 <span>$$a>0$$</span>이면 Positive, <span>$$a<0$$</span>이면 Negative로서 판단할 수 있다.

### 5.2 Maximizing the Margin

Margin을 Maximizing하기 위하여 먼저 Margin을 구하여보자.  
먼저 위의 그림을 다시 한번 생각하고 용어를 정리하여 보자.
- <span>$$x_p$$</span>: Hyperplane위의 Point
- <span>$$f(x) = wx+b = a \text{  or  } -a$$</span>: Support Vector
- <span>$$w$$</span>: Hyperplane의 접선 Vector
- <span>$$r$$</span>Hyperplane의 에서 Support Vector 까지의 거리

위와 같이 가정하였을 경우 다음과 같이 x를 정의할 수 있다.
<p>$$x=x_p + r\frac{w}{||w||}$$</p>

즉, 임의의 한 Point는 Hyperplane위의 Point를 기준으로 방향은 w, 크기는 r인 Vector로서 표현할 수 있는 것 이다.

위의 식을 활용하면 다음과 같다.  
<p>$$f(x) = w(x_p + r \frac{w}{||w||})+b = r||w|| (\because f(x_p) = wx_p+b = 0)$$</p>
<p>$$\therefore r = \frac{f(x)}{||w||} \rightarrow \text{margin} = \frac{2a}{||w||}$$</p>

위에서 Support Vector의 값을 a라고 하였으므로 다음과 같이 표현할 수 있다.  
<p>$$max_{w,b}2r = \frac{2a}{||w||}$$</p>
<p>$$s.t(wx_j+b)y_j \ge a$$</p>

위의 식에서 a는 임의의 상수이므로 a=1이라 가정하면 최종적인 Maximizing the Margin은 다음과 같이 나타낼 수 있다.  

$$min_{w,b}||w||$$
$$s.t.(wx_j+b)y_j \ge 1$$

이러한 방법은 Hard Margin이라고 불리게 된다. Hard Margin이란 Linear한 Line으로서 어떻게든 Data를 분류한다는 의미이다.  

아래와 같은 Data Point 분포가 있을 경우 문제를 생각해 보자.  
![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Theory/12.png)

Decision Boundary와 Classify의 기준이 되는 Line사이의 Data Point가 들어오는 경우, 혹은 Linear한 Line으로는 도저히 분류할 수 없는 상황 등이 있다.  

위와 같은 Error상황에서 크게 2가지의 해결방안이 있다.

1. Soft Margin: Error를 어느정도 허용하고 Linear하게 분류한다.
2. Kernel Trick: Non Linear하게 분류한다.

### 5.3 Error Handling in SVM
위에서 두가지 방법에 대하여 다음과 같이 정의 할 수 있다.

**1. Soft Margin**
- **Admin there will be an "error"**
- Represent the error in our problem formulation
- Try to reduce the error as well

**2. Kernel Trick**
- **Make decision boundary**
- more complex
- Go to **non-linear**

Soft Margin과 Kernel Trick둘다, 위에서 선언한 <span>$$min_{w,b}||w||,  s.t(wx_j+b)y_j \ge 1$$</span>로서는 Error를 해결할 수 없으므로 **Error에 관한 값을 Penalty Function으로서 나타내고 Penalty Function의 값 또한 Minimize하는 값을 찾아서 Classify하게 한다는 것 이다.**  

즉, 어느정도 Error는 허용하나 그 Error가 최소화되게 만든다는 의미이다.  

Penalty Function을 선언하는 방법은 2가지가 있다.

**Option 1**  
<p>$$min_{w,b}||w||+C*(\text{Num of Error})$$</p>
<p>$$s.t(wx_j+b)y_j \ge 1$$</p>
위의 식을 살펴보게 되면 Error의 개수 * C(임의의 상수)만큼 Penalty를 준다는 것 이다.  
이러한 Penalty Function은 0-1 Loss라고 부르게 된다.  
**0-1 Loss란 Hyperplane까지는 Penalty가 없으나 반대쪽의 Support Vector를 넘어가는 순간 1의 Penalty를 주는 Penalty Function이다. 하지만, 이러한 Function은 거리와 상관없이 Count만으로서 Penalty를 주기 때문에 정확한 방법이라고 할 수 없다.**

**Option2**  
<p>$$min_{w,b}||w||+C\sum_{j}\xi_j$$</p>
<p>$$s.t(wx_j+b)y_j \ge 1-\xi_j$$</p>
<p>$$\xi_j \ge 0$$</p>
<p>$$\text{EX)  } \xi_j =(1-(wx_j+b)y_j)_{+}$$</p>

위의 식을 살펴보게 되면 <span>$$\xi$$</span>는 Distance에 따라 <span>$$\xi_j$$</span>만큼 Penalty를 주겠다는 의미이다.  
**0-1 Loss에서의 문제점이였던, 거리에 따른 가중치를 주지 않는 것은 해결하였다. 이러한 Loss Function은 Hinge Loss라고 불린다. 하지만, C라는 Constant를 정의해야 하는 새로운 문제가 발생하게 되었다.**

위의 Option1,2의 Penalty Function을 SVM에 적용하여 Visualizatio하면 다음과 같이 나타낼 수 있다.
![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Theory/13.png)

### 5.3 Soft Margin with SVM
**Soft Margin**
- **Admin there will be an "error"**
- Represent the error in our problem formulation
- Try to reduce the error as well

Soft Margin과 Kernel Trick둘다, 위에서 선언한 <span>$$min_{w,b}||w||,  s.t(wx_j+b)y_j \ge 1$$</span>로서는 Error를 해결할 수 없으므로 **Error에 관한 값을 Penalty Function으로서 나타내고 Penalty Function의 값 또한 Minimize하는 값을 찾아서 Classify하게 한다는 것 이다.**  

즉, 어느정도 Error는 허용하나 그 Error가 최소화되게 만든다는 의미이다.  

Penalty Function을 선언하는 방법은 2가지가 있다.

**Option 1**  
<p>$$min_{w,b}||w||+C*(\text{Num of Error})$$</p>
<p>$$s.t(wx_j+b)y_j \ge 1$$</p>
위의 식을 살펴보게 되면 Error의 개수 * C(임의의 상수)만큼 Penalty를 준다는 것 이다.  
이러한 Penalty Function은 0-1 Loss라고 부르게 된다.  
**0-1 Loss란 Hyperplane까지는 Penalty가 없으나 반대쪽의 Support Vector를 넘어가는 순간 1의 Penalty를 주는 Penalty Function이다. 하지만, 이러한 Function은 거리와 상관없이 Count만으로서 Penalty를 주기 때문에 정확한 방법이라고 할 수 없다.**

**Option2**  
<p>$$min_{w,b}||w||+C\sum_{j}\xi_j$$</p>
<p>$$s.t(wx_j+b)y_j \ge 1-\xi_j$$</p>
<p>$$\xi_j =(1-(wx_j+b)y_j)_{+} \rightarrow \xi_j \ge 0$$</p>

위의 식을 살펴보게 되면 <span>$$\xi$$</span>는 Distance에 따라 <span>$$\xi_j$$</span>만큼 Penalty를 주겠다는 의미이다.  
**0-1 Loss에서의 문제점이였던, 거리에 따른 가중치를 주지 않는 것은 해결하였다. 이러한 Loss Function은 Hinge Loss라고 불린다. 하지만, C라는 Constant를 정의해야 하는 새로운 문제가 발생하게 되었다.**

위의 Option1,2의 Penalty Function을 SVM에 적용하여 Visualizatio하면 다음과 같이 나타낼 수 있다.
![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Theory/13.png)


**새로운 Constant C에 대하여 값을 변경시키면서 결과를 출력하면 다음과 같은 결과를 얻을 수 있다.**  
![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Theory/14.png)

**Data가 Linear하게 Classify되지 않는다고 가정하였을 경우, Penalty가 작으면 여전히 잘 분류하지 못하지만, 일정 크기 이상일 경우에는 어느정도의 Error를 감수하고 잘 분류하는 것을 살펴볼 수 있다. 이러한 C의 값은 무조건 크면 좋은 것이 아니라, 우리가 사용하고자 하는 Data의 신뢰도에 따라 Penalty를 줘야지 의미있는 Classification Model(SVM)을 설계할 수 있을 것 이다.**

### 5.4 Comparison to Logistic Regression
위의 SoftMargin SVM에서 LossFunction을 Hinge Loss로서 사용하였다.  

이러한 LossFunction은 이전에 배웠던 Logistic Regression에서도 사용하였다.  
<a href="https://wjddyd66.github.io/machine%20learning/Theory(4)Logistic-Regression/#43-logistic-regression-parameter-approximation-1">4.3 Logistic Regression Parameter Approximation 1</a>에서 Logistic Regression식 을 생각해보자.

<p>$$\hat{\theta} = \argmax_{\theta} \sum_{i=1}^N log(P(Y_i|X_i ;\theta)) = \argmax_{\theta}\sum_{i=1}^{N}(Y_iX_i\theta - log(1+e^{X_i\theta}))$$</p>

위의 식을 살펴보게 되면 LossFunction은 <span>$$log(1+e^{X_i\theta})$$</span>가 되는 것을 살펴볼 수 있다.

최종적인 0-1, Hinge, Log Loss는 다음과 같이 나타낸다.  
<img src="http://fa.bianp.net/blog/static/images/2013/loss_functions.png"><br>
사진 출처: <a href="http://fa.bianp.net/blog/2013/loss-functions-for-ordinal-regression/">fa.bianp</a>

위의 Loss를 생각해보면 다음과 같은 의미를 가지고 있다.  
**SVM의 경우 Hyperplane을 기준으로서 Prediction이 맞다면 Penalty가 0이다. 즉, 완벽히 Classification가능하다고 생각한다는 것 이다.**  

**Logistic Regression의 Log Loss Function을 살펴보게 되면, 아주 잘못되어도 0이아닌 값을 가지게 된다.(0에 매우 가까울 것 이다.)즉, 아주 0에 가까운 확률로서 Prediction이 잘못되었다고 판단할 수 있다는 것 이다.**

### 5.5 Rethinking of SVM
**Kernel Trick**
- **Make decision boundary**
- more complex
- Go to **non-linear**

Error를 해결하기 위하여 2번째 방법인 Kernel을 이용하는 방법이다.  
Kernel하나의 예시를 식으로서 표현하면 다음과 같다.  
<p>$$\varphi(<x_1,x_2>) = <x_1, x_2, x_1^2, ..., x_1x_2^2>$$</p>

**즉, Kernel Trick이란 Data의 Dimension을 늘리는 방법이다.**  

Kernel Trick을 사용하기 위하여 Duality -> KKT Condition을 사용하고 이를 SVM에 적용하자.

**Lagrange multiplier**
<img src="https://t1.daumcdn.net/cfile/tistory/242B713357C84A3402">

$$L(x,y,\lambda) = f(x,y) - \lambda(g(x,y)-c)$$
$$\frac{\partial f}{\partial x} = \lambda \frac{\partial g}{\partial x}$$
$$\frac{\partial f}{\partial y} = \lambda \frac{\partial g}{\partial y}$$

f(x,y): Maximum지점을 찾기 위한 함수, g(x,y) = c는 조건  

위의 식에서 구하고자 하는 함수와 조건을 넣으면 다음과 같이 Object Function을 얻을 수 있다.  
### Duality
위에서 설명한 Lagrange Multiplier를 사용하여 Primal -> Duality문제로 변형하여 보자.  
얻고자 하는 것은 object Function이 Minimize되는 것 이다.  
이것은 다음과 같이 나타낼 수 있다.  

**Step 1**  
<p>$$min_{x} f(x)$$</p>
<p>$$\text{subject to  }g(x) \le 0, h(x)=0$$</p>
위의 식을 Largrange Multiplier를 적용하면 다음과 같이 변형할 수 있다.  

<p>$$L(x,\alpha,\beta) = f(x)+\alpha g(x) + \beta h(x)$$</p>
만약 <span>$$\alpha \ge 0$$</span>이라면 아래 식이 성립하는 것을 알 수 있다.  
<p>$$f^{*} \ge min_{x \in C} L(x,\alpha,\beta) \ge min_{x} L(x,\alpha,\beta)$$</p>
<p>$$C: \alpha \ge 0$$: 이라는 조건이 없을 경우 x의 집합</p>
<p>$$f^{*}$$: 찾고자하는 Optimal 한 값</p>

**Step 2**  
Primal -> Duality로 변형하기 위하여 Largrange Function을 미분한다.
사용하기 위한 각각의 Function이 다음과 같을 경우  
- <span>$$f(x) = cx$$</span>
- <span>$$g(x) = Gx-g \le 0$$</span>
- <span>$$h(x) = Hx-h = 0$$</span>

위와 같이 정리하면 식을 다음과 같이 정의할 수 있다.
<p>$$\frac{\partial L}{\partial x}= c+\alpha G+\beta H = 0 \rightarrow c = -\alpha G -\beta H$$</p>

위의 식을 맨 처음 Largrange Function에 대입하면 다음과 같은 결과가 나온다.
<p>$$L(x,\alpha,\beta) = f(x)+\alpha g(x) + \beta h(x)$$</p>
<p>$$= (-\alpha G -\beta H)x + \alpha(Gx-g)+\beta(Hx-h) = -\alpha g - \beta h$$</p>

**위의 식은 더이상 <span>$$x$$</span>에 관한 식이 아닌 <span>$$\alpha,\beta$$</span>에 관한 식이다. 위의식 <span>$$-\alpha g - \beta h = d(\alpha,\beta)$$</span>라고 한다면 Largrange Function은 다음과 같다.**  

**Step 3**  
<p>$$f^{*} \ge d(\alpha,\beta)$$</p>
위의 식이 성립하므로, Optimal한 값의 Minimize하는 것은 Duality Probelm으로 나타내었을 때 Function을 Maximize하는 것과 동일하게 된다.

**즉, Primal -> Duality가 되면서 Minimize가 Maximize가 되었고, x에 관한 식이 <span>$$\alpha,\beta$$</span>에 관한 식으로서 변형되었다.**  

**최종적인 식은 다음과 같다.**  
<p>$$ max_{\alpha \ge 0,\beta} d(\alpha,\beta)$$</p>
<p>$$\text{s.t.  }\alpha \ge 0$$</p>

**Duality GAP**  
<p>$$f^{*} \ge min_{x \in C} L(x,\alpha,\beta) \ge min_{x} L(x,\alpha,\beta) = d(\alpha,\beta)$$</p>

위의 식에서 항상 등호가 성립할 수는 없다.  
**즉, 최종적인 식(<span>$$d(\alpha,\beta)$$</span>)은 Primal Problem에서의 Lower Boundary로서 Maximization하면 비슷한 값이 나오겠다이지, 등호가 반드시 성립은 하지 않는다. 이에 따른 실제 Optimal의 값(<span>$$f^{*}$$</span>)과의 차이를 duality gap이라고 한다.**

### 5.6 Primal and Dual with KKT Condition
**KKT Condition이란, 이러한 Condition을 만족하는 경우 위에서 언급한 Duality Gap이 없어지고 Strong Duality이 되는 조건이다.**  

먼저 KKT의 조건을 살펴보면 각각 다음과 같다.  
- <span>$$\partial L(x^{*},\alpha^{*},\beta^{*}) = 0$$</span>: Lagrange Multiplier Condition
- <span>$$\alpha^{*} \ge 0$$</span>: Primal -> Duality Condition
- <span>$$g(x^{*}) \le 0$$</span>: Primal Condition
- <span>$$h(x^{*}) = 0$$</span>: Primal Condition
- <span>$$\alpha^{*}g(x^{*}) = 0$$</span>: **New Condition**

위의 5가지 조건에서 새롭게 추가된 조건은 5번째 조건이다.  
즉, 1~4는 기존의 Primal, Duality Problem을 해결하기 위한 Condition이었고, 5번째 새로운 Condition이 추가가 되면 어떻게 KKT Condition이 성립하여 Duality Gap이 없어지는지 알아보자.

위의 Duality에서 사용한 식을 살펴보면 다음과 같다.  
1. <span>$$f^{*} \ge min_{x \in C} L(x,\alpha,\beta) \ge min_{x} L(x,\alpha,\beta) = max_{\alpha \ge 0,\beta} d(\alpha,\beta)$$</span>
2. <span>$$L(x,\alpha,\beta) = f(x)+\alpha g(x) + \beta h(x)$$</span>
3. <span>$$f(x^{*}) = d(\alpha^{*},\beta^{*})$$</span>(Strong Duality라면)

**Duality Problem**에서 전부 등호가 되기 위해서는 다음과 같은 조건이 성립하여야 한다.  
<p>$$d(\alpha^{*},\beta^{*}) = min_{x}L(x,\alpha^{*},\beta^{*}) = L(x^{*},\alpha^{*},\beta^{*}) = f(x^{*})$$</p>

**Primal Problem**에서 전부 등호가 되기 위해서는 다음과 같은 조건이 성립하여야 한다.  
<p>$$L(x^{*},\alpha^{*},\beta^{*}) = f(x^{*})+\alpha^{*} g(x^{*}) + \beta^{*} h(x^{*}) = f(x^{*})$$</p>

위의 두 식을 1번에 넣고 부등호를 유지하면 다음과 같다.  
<p>$$L(x^{*},\alpha^{*},\beta^{*}) \le f(x^{*})+\alpha^{*} g(x^{*}) + \beta^{*} h(x^{*}) \le f(x^{*})$$</p>
<p>$$\therefore f(x^{*}) \le f(x^{*}) +\alpha^{*} g(x^{*}) + \beta^{*} h(x^{*}) \le f(x^{*})$$</p>

**최종적인 식을 얻기 위해서는 부등호 사이에 있는 <span>$$\alpha^{*} g(x^{*}) + \beta^{*} h(x^{*})$$</span>값이 0이 되어야 하나, Primal Condition에서 <span>$$h(x^{*})=0$$</span>이라 가정하였기 때문에 <span>$$\alpha^{*} g(x^{*})$$</span>을 만족하면 된다.**  

즉, 위에서 새롭게 추가된 New Condition을 만족하게 되면 Primal -> Duality + Strong Duality가 만족하게 된다.

<img src="https://drive.google.com/uc?id=1oNZ5gyNxFMJg1TvQshCc1bY8QlvSDLJO"><br>
사진 참조: <a href="https://ratsgo.github.io/convex%20optimization/2018/01/26/KKT/">ratsgo 블로그</a>


### Dual Representation of SVM
위의 Duality + KKT Condition을 SVM에 적용하면 다음과 같다.  

<p>$$min_{w,b}||w||$$</p>
<p>$$s.t(wx_j+b)y_j \ge 1$$</p>

<p>$$L(w,b,\alpha) = \frac{1}{2}ww-\sum_{j}\alpha_j[(wx_j+b)y_j-1]$$</p>

위의 식에서 KKT Condition의 조건을 차례대로 적용하면 다음과 같다.  

**1. <span>$$\partial L(x^{*},\alpha^{*},\beta^{*}) = 0$$</span>: Lagrange Multiplier Condition**  
- <span>$$\frac{\partial L(w,b,\alpha)}{\partial w} \rightarrow w = \sum_{i=1}^{n} \alpha_i y_i x_i$$</span>
- <span>$$\frac{\partial L(w,b,\alpha)}{\partial b} \rightarrow 0 = \sum_{i=1}^{n} \alpha_i y_i$$</span>

**2. <span>$$\alpha^{*} \ge 0$$</span>: Primal -> Duality Condition**
<p>$$\alpha_j \ge 0$$</p>

**3. <span>$$\alpha^{*}g(x^{*}) = 0$$</span>: New Condition**
<p>$$\alpha_i ((wx_j+b)y_j-1)=0$$</p>

**참조**  
1. 자세한 수식 유도: <a href="https://kaist.edwith.org/machinelearning1_17/lecture/10605/">문일철 머신러닝 강의</a>
2. Error를 허용하는 SVM Dual Representation: <a href="https://ratsgo.github.io/convex%20optimization/2018/01/26/KKT/">ratsgo 블로그</a>

### 5.7 Kernel

Kernel이라는 것은 Mapping Function을 사용하여 Data의 차원을 늘려서 Linear하게 Classify를 못하는 문제를 Linear 하게 Classify하게 바꾼다는 의미이다.  

XOR의 예시를 살펴보게 되면 다음과 같다.  
![png](https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/HandsOn/Theory/15.png)

Linear로서 Classify하지 못했던 문제를 Mapping Function인 <span>$$\pi(x)$$</span>를 통하여 Data를 2->3차원으로서 변형시키고 Linear하게 Classify하게 한다.  

이러한 간단한 Mapping Function을 사용하게 되면, Linear하게 계속하여 Space가 증가하게 된다. 따라서 이러한 Space를 작게 하고, 연산의 양을 줄이기 위하여 **Kernel을 사용하게 된다.**  

먼저 Kernel의 식을 살펴보면 다음과 같다.
<p>$$K(x_i,x_j) = \varphi(x_i) \cdot \varphi(x_j)$$</p>

각각의 Data의 차원을 이동하는 <span>$$\varphi$$</span>의 내적으로서 표현한다는 것 이다.  
이러한 Kernel의 대표적인 종류를 생각하면 다음과 같다.  

- Polynomial: <span>$$k(x_i,x_j) = (x_i \cdot x_j)^d$$</span>
- Gaussian: <span>$$k(x_i,x_j) = exp(-r||x_i - x_j||^2) (\text{단,  }r = \frac{1}{2 \sigma^2})$$</span>
- Hyperbolic tangent: <span>$$k(x_i,x_j) = tanh(kx_i \cdot x_j+c) (\text{단,  }k > 0, c<0 )$$</span>

위에서 Polynormial일때를 생각하면 다음과 같이 나타낼 수 있다.
**Polynomial Function Degree 1**  
<p>$$K(<x_1,x_2>,<z_1,z_2>) = <x_1,x_2> \cdot <z_1,z_2> = x_1z_1+x_2z_2 = x \cdot z$$</p>

**Polynomial Function Degree 2**  
<p>$$K(<x_1,x_2>,<z_1,z_2>) = <x_1^2,\sqrt{2}x_1x_2,x_2^2> \cdot <z_1^2,\sqrt{2}z_1z_2,z_2^2> $$</p>
<p>$$= x_1^2z_1^2+2x_1x_2z_1z_2+x_2^2z_2^2 = (x \cdot z)^2$$</p>
...

즉, Input으로 들어가는 Vector를 내적한다음 그 값을 곱하는 형식으로서 Data의 Dimension을 늘릴 수 있다.  

**이러한 특성으로서 Kernel Function은 1차원의 Data를 100차원으로 늘리는 경우 101번의 연산이 필요할 뿐이므로, 원하는 차원으로서 변형하는데 계산되는 양을 매우 줄이면서 Dimension을 늘릴 수 있다. => Dimension을 바꿈으로 인하여 Non-Linear하게 문제를 해결하는 것 처럼 만들 수 있다.**

### 5.8 SVM with Kernel
위에서 최종적으로 얻은 식을 생각하면 다음과 같이 적용할 수 있다.  

<p>$$w = \sum_{i=1}^{N}\alpha_iy_ix_i$$</p>
<p>$$b = y_j - \sum_{i=1}^{N} \alpha_iy_ix_ix_j$$</p>
<p>$$\because y_j = w_jx_j+b_j$$</p>

위의 식에서 Kernel을 적용하면 다음과 같이 나타낼 수 있다.
<p>$$w = \sum_{i=1}^{N}\alpha_iy_i\varphi(x_i)$$</p>
<p>$$b = y_j - \sum_{i=1}^{N} \alpha_iy_i\varphi(x_i)\varphi(x_j) = y_j - \sum_{i=1}^{N} \alpha_iy_iK(x_i,x_j)$$</p>

**위의 w는 Kernel Trick 형식이 아닌 Mapping Function이다. 하지만 우리는 결국에 SVM의 식을 생각해보자. 즉, SVM은 궁극적으로 Input에 대하여 양수인지 음수인지로 Classification하는 Model이다.**  

식으로서 생각하면 다음과 같다.  
<p>$$sign(w \cdot x +b)$$</p>
위의 식에 정리한 식을 대립하면 다음과 같은 결과를 얻게 된다.
<p>$$sign(w \cdot \varphi(x) +b) = sign(\sum_{i=1}^{N}\alpha_iy_i\varphi(x_i)\cdot \varphi(x) + y_j - \sum_{i=1}^{N} \alpha_iy_iK(x_i,x_j))$$</p>
<p>$$= sign(\sum_{i=1}^{N}\alpha_iy_i K(x_i,x) + y_j - \sum_{i=1}^{N} \alpha_iy_iK(x_i,x_j))$$</p>

즉, 실제 Model의 결과에서는 Kernel형태로 들어가게 되므로 문제없이 사용할 수 있다.

**결과적으로 SVM은 Kernel Trick을 활용하여 Linear에서 해결하지 못하였던 문제를 NonLinear하게 해결할 수 있는 Classification Model로서 바뀌게 된다.**
